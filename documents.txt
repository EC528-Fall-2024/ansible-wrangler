--- # Ansible playbook for setting up the Openconext-Stepup infrastructure # Run: # cd Stepup-Deploy # ansible-playbook -i <environment_directory>/inventory site.yml   - hosts: all   run_once: true   tasks:   - name: Require Ansible 2     assert:       that: ansible_version.major == 2   - name: Configuration for all nodes   hosts: all   become: True   tags: common   serial: 1    roles:   - common   - name: Setup management server   hosts: manage   become: True   tags: manage   serial: 1    roles:   - manage   - name: Setup mariadb database cluster   hosts: dbcluster   become: True   tags: db   serial: 1    roles:   - db   - name: Setup elasticseach cluster   hosts: es   become: True   tags: es   serial: 1    roles:     - es   - name: Setup application servers   hosts: app   become: True   tags: app   serial: 1    roles:   - app   - name: Setup proxy on application servers   hosts: proxy   become: True   tags: proxy   serial: 1    roles:   - proxy   # Add development tooling to app server # not for production use - name: Dev tools   hosts: dev   become: True   tags: dev   serial: 1    roles:   - dev   
######################################################################################### # #  Ansible Dev Machine Setup - Kubuntu #  Playbooks to not run as sudo #########################################################################################  - name: Configure machine for development work   hosts: localhost   connection: local  ##### Core tools that are missing from Kubuntu  - import_playbook: sdkman/core.yml   # Final tasks - import_playbook: java/core.yml  ### Customizations - import_playbook: bash/core.yml - import_playbook: bash/post.yml - import_playbook: kde.yml 
# ansible-playbook --ask-sudo-pass -i "localhost," -c local work.yml --- - hosts: localhost   connection: local    become_user: root   become: true    vars:     rstudio_version: 0.99.484-amd64    tasks:      # R      - name: Add CRAN GPG key       tags: R       apt_key: id=0xE084DAB9 keyserver=keyserver.ubuntu.com      - name: Add CRAN repository       tags: R       apt_repository: repo='deb http://cran.rstudio.com/bin/linux/ubuntu {{ ansible_distribution_release }}/'      - name: Install R       tags: R       apt: name=r-base state=latest      # TODO: Fixing unknown error R failing when run through Ansible, but     #       commands function properly on the command line.     #     # - name: R dependencies     #   shell: Rscript -e "if (!('{{ item }}' %in% installed.packages()[,'Package'])) { install.packages('{{ item }}', repos='http://cran.rstudio.com/', quiet=TRUE); }; if (!('{{ item }}' %in% installed.packages()[,'Package'])) { stop('{{ item }} was NOT installed'); };"     #   with_items:     #     - SDMTools     #     - weights     #     - ggplot2     #     - zoo     #     - dplyr     #     - forecast     #     - fitdistrplus     #     - fpc     #     - truncdist     #     - dgof     #     # sudo R -e "if (!('devtools' %in% installed.packages()[,'Package'])) { install.packages('devtools', repos='http://cran.rstudio.com/', quiet=TRUE); }; if (!('devtools' %in% installed.packages()[,'Package'])) { stop('devtools was NOT installed'); };"     # sudo R --quiet -e "if(!('osmutil' %in% installed.packages()[, 'Package'])) { devtools::install_git('git@git.orm-tech.com:ORM/osmutil.git', branch = 'v1.0.0', quiet=TRUE); }; if (!('osmutil' %in% installed.packages()[,'Package'])) { stop('osmutil was NOT installed'); };"      # RStudio      - name: RStudio dependencies       tags: rstudio       apt: name={{ item }} state=latest       with_items:         - libjpeg62      - name: RStudio downloading version {{ rstudio_version }}       tags: rstudio       get_url: url=https://download1.rstudio.org/rstudio-{{ rstudio_version }}.deb                dest=/opt/rstudio-{{ rstudio_version }}.deb                validate_certs=no      - name: Install RStudio       tags: rstudio       apt: deb=/opt/rstudio-{{ rstudio_version }}.deb      # VirtualBox      - name: VirtualBox       tags: virtualbox       apt: name={{ item }} state=latest       with_items:         - virtualbox 
# Ansible Playbook for Subutai Jenkins Blueprint ---  - hosts: jenkins   remote_user: root   tasks:      - name: Upgrade debian       apt:         update_cache: true         upgrade: true      - name: Install required debs       apt:         name: "{{ packages }}"       vars:          packages:         - default-jre         - ca-certificates         - xz-utils         - python3-apt       #    - name: Add an Apt signing key #      apt_key: #        url: https://pkg.jenkins.io/debian/jenkins.io.key #        state: present  #    - name: Add jenkins repository #      apt_repository: #        repo: deb http://pkg.jenkins.io/debian-stable binary/ #        state: present #        update_cache: true  #    - name: Install jenkins #      apt:  #        name: jenkins #        state: present #        update_cache: yes      - name: Install a jenkins 2.164.3.deb package from the internet.       apt:         deb: https://ipfs.subutai.io/ipfs/Qmf3svqv8WJSU42q1T3751Jr9oaKDexgrzNWyxa4bEiRmb/jenkins_2.164.3_all.deb      - name: Wait until the file "initialAdminPassword" is present before continuing       wait_for:         path: /var/lib/jenkins/secrets/initialAdminPassword         - name: read admin pwd       shell: cat /var/lib/jenkins/secrets/initialAdminPassword       register: adminpwd      - name: Wait for port 8080 to become open on the host, don't start checking for 10 seconds       wait_for:         port: 8080         delay: 30      - name: unlock and add admin user       jenkins_script:         script: |           import jenkins.model.*           import hudson.security.*           def instance = Jenkins.getInstance()           def hudsonRealm = new HudsonPrivateSecurityRealm(false)           hudsonRealm.createAccount('admin', '${user_pwd}')           instance.setSecurityRealm(hudsonRealm)           def strategy = new FullControlOnceLoggedInAuthorizationStrategy()           strategy.setAllowAnonymousRead(false)           instance.setAuthorizationStrategy(strategy)           instance.save()         args:           user_pwd: "{{ admin_user_pass }}"         user: admin         password: "{{ adminpwd.stdout }}"      - name: complete setup wizard       jenkins_script:         script: |           import static jenkins.model.Jenkins.instance as jenkins           import jenkins.install.InstallState           if (!jenkins.installState.isSetupComplete()) {             InstallState.INITIAL_SETUP_COMPLETED.initializeState()           }         user: admin         password: "{{ admin_user_pass }}"      # - name: Change Jenkins JAVA options     #   lineinfile:      #     dest: /etc/default/jenkins     #     regexp: ^JENKINS_ARGS=     #     line: JENKINS_ARGS="--webroot=/var/cache/$NAME/war --httpPort=$HTTP_PORT --httpListenAddress=127.0.0.1"      - name: Restart jenkins       systemd:          name: jenkins.service         state: restarted 
#!/usr/bin/env ansible-playbook --- - hosts: vdo_hosts    tasks:     - name: vdo packages       yum:         name:           - vdo           - kmod-kvdo         state: latest      - name: vdo service       service:         name: vdo         state: started         enabled: true  - hosts: ogre   vars:     partition_num: 4     luks_device: "/dev/sda{{ partition_num }}"     luks_keyfile_path: /root/luks.keyfile     luks_name: rhv_luks     mount_point: /rhv_data     vdo_devname: rhv_data   vars_files:     - passwords.vault    tasks:     - name: fail if vault not decrypted       fail: msg="Vault not decrypted"       when: luks_password is undefined      - name: disk_data       parted:         device: /dev/sda         unit: B       register: sda_info      - name: show_data       debug:         var: sda_info      - name: luks_partition       parted:         device: /dev/sda         number: "{{ partition_num }}"         label: gpt         name: "{{ luks_name }}"         unit: B         state: present         part_start: "{{ sda_info.partitions[partition_num-2].end + 1}}B"         part_end: "{{ sda_info.partitions[partition_num-2].end + 2199023255552 + 1 }}B" # 2TB      - name: luks_keyfile       copy:         content: "{{ luks_password }}"         dest: "{{ luks_keyfile_path }}"         mode: u+rw,go=      - name: luks_device       luks_device:         device: "{{ luks_device }}"         name: "{{ luks_name }}"         state: "opened"         keyfile: "{{ luks_keyfile_path }}"      - name: unlock_during_boot       crypttab:         name: "{{ luks_name }}"         state: present         opts: discard         backing_device: "{{ luks_device }}"      - name: remove_luks_keyfile       file:         dest: "{{ luks_keyfile_path }}"         state: absent      - name: vg_for_RHV       lvg:         vg: rhv_vg         pvs: "/dev/mapper/{{ luks_name }}"      - name: lv_for_vdo       lvol:         vg: rhv_vg         lv: vdo_lv         size: 100%VG      # needs https://github.com/ansible/ansible/issues/54556     - name: vdo_volume       vdo:         name: "{{ vdo_devname }}"         state: present         running: true         device: /dev/mapper/rhv_vg-vdo_lv         logicalsize: 3T      - name: xfs format       filesystem:         dev: "/dev/mapper/{{ vdo_devname }}"         fstype: xfs         resizefs: true      - name: determine_mount_name       command: systemd-escape -p --suffix=mount {{ mount_point }}       register: mount_filename       changed_when: false      - name: vdo mount       template:         src:  vdo/vdo.j2         dest: "/etc/systemd/system/{{ mount_filename.stdout }}"       vars:         vdo_devname: rhv_data         fstype: xfs      - name: vdo mounted       service:         name: "{{ mount_filename.stdout }}"         state: started         enabled: true  - hosts: beast   vars:     mount_point: /var/lib/libvirt/images     vdo_devname: vdovol    tasks:     - name: vdolv       lvol:         vg: rootvg         lv: vdolv         size: 500G      # needs https://github.com/ansible/ansible/issues/54556     - name: vdo volume       vdo:         name: "{{ vdo_devname }}"         state: present         running: true         device: /dev/mapper/rootvg-vdolv         logicalsize: 1.5T      - name: xfs format       filesystem:         dev: "/dev/mapper/{{ vdo_devname }}"         fstype: xfs         resizefs: true      - name: determine_mount_name       command: systemd-escape -p --suffix=mount {{ mount_point }}       register: mount_filename       changed_when: false      - name: show_mount_name       debug:         var: mount_filename.stdout      - name: vdo mount       template:         src:  vdo/vdo.j2         dest: /etc/systemd/system/{{ mount_filename.stdout }}       vars:         fstype: xfs      - name: vdo mounted       service:         name: "{{ mount_filename.stdout }}"         state: started         enabled: true  ... 
--- # This playbook deploys the configuration based on special purpose host-groups.  # devtest hosts correspond to the dev, lab and vagrant based machines for contiv projects. # Here we will install the base packages needed for those environments that includes: # - base production packages like python, ansible, serf etc # - development tools like go # - pre-bake some binaries that are otherwise installed as part of contiv #   service deployments like collins, sky-dns, ceph etc. - hosts: devtest   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: dev }   - { role: test }  - hosts: volplugin-test   become: true   environment: '{{ env }}'   roles:    - { role: bind }   - { role: base }   - { role: nfs }   - { role: vagrant }   - { role: ucarp }   - { role: docker, etcd_client_port1: 2379 }   - { role: consul }   - { role: etcd, run_as: master }   - { role: ceph-mon, mon_group_name: volplugin-test }   - { role: ceph-osd, mon_group_name: volplugin-test, osd_group_name: volplugin-test }   - { role: scheduler_stack, run_as: master }   - { role: contiv_network, run_as: master }   - { role: gluster, gluster_peers_group: volplugin-test, gluster_interface: "{{ control_interface }}"}  # cluster-node hosts corresponds to the hosts that shall be managed by cluster manager. # This host group shall provision a host with all required packages needed to make # the node ready to be managed by cluster-manager - hosts: cluster-node   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: serf }  # cluster-control hosts corresponds to the first machine in the cluster that is provisioned # to bootstrap the cluster by starting cluster manager and inventory database (collins) - hosts: cluster-control   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: serf }   - { role: docker, etcd_client_port1: 2379 }   - { role: contiv_cluster }  # service-master hosts correspond to cluster machines that run the master/controller # logic of the infra services - hosts: service-master   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: ucarp }   - { role: docker }   - { role: etcd, run_as: master }   - { role: nfs }   # - { role: ceph-mon, mon_group_name: service-master, when: host_capability|match('.*can-run-user-containers.*') }   # - { role: ceph-osd, mon_group_name: service-master, osd_group_name: service-master, when: host_capability|match('.*storage.*') }   - { role: scheduler_stack, run_as: master, when: host_capability|match('.*can-run-user-containers.*') }   - { role: contiv_network, run_as: master, when: host_capability|match('.*can-run-user-containers.*') }   - { role: contiv_storage, run_as: master }  # service-worker hosts correspond to cluster machines that run the worker/driver # logic of the infra services. - hosts: service-worker   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: docker }   - { role: etcd, run_as: worker }   - { role: nfs }   # - { role: ceph-osd, mon_group_name: service-master, osd_group_name: service-worker, when: host_capability|match('.*storage.*') }   - { role: scheduler_stack, run_as: worker, when: host_capability|match('.*can-run-user-containers.*') }   - { role: contiv_network, run_as: worker, when: host_capability|match('.*can-run-user-containers.*') }   - { role: contiv_storage, run_as: worker }  # netplugin-node hosts set up netmast/netplugin in a cluster - hosts: netplugin-node   become: true   environment: '{{ env }}'   roles:   - { role: base }   - { role: docker, etcd_client_port1: 2379 }   - { role: etcd, run_as: master }   - { role: scheduler_stack, run_as: master }   - { role: contiv_network, run_as: master } 
---  - name: Ansible Playbook for installing nifi   hosts: master   remote_user: "{{ deployer }}"   vars_files:     - vars/common.yml   roles:     - nifi 
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.dns       # Added `dns_zones`.       dns_zones:         # Remove the trailing period (.).         - name: lab.controlplane.info           ttl: 600           ns:             # Added a trailing period (.).             - name: ns.lab.controlplane.info.           mx:             - name: mail1.lab.controlplane.info.               priority: 10             - name: mail2.lab.controlplane.info.               priority: 20           records:             - name: ns               value: 192.168.254.27             - name: git               value: 192.168.254.19             - name: dl380               value: 192.168.254.27             # Added mail1 because is't mentioned in the mx records             - name: mail1               value: 192.168.123.123             # Added mail2 because is't mentioned in the mx records             - name: mail2               value: 192.168.123.123         # Remove the trailing period (.).         - name: forwarded.lab.controlplane.info           ns:             - name: forwarded.lab.controlplane.info.           records:             - name: ns               value: 192.168.254.27             # Added ns because it's mentioned in the ns records             - name: "@"               value: 192.168.123.123           dns_zone_forwarders:             - 9.9.9.9             - 8.8.8.8       # An optional list of acls to allow recursion. ("any" and "none" are always available.)       dns_allow_recursion:         - none       # An optional setting to forward traffic to other DNS servers.       dns_options_forwarders:       #   - 1.1.1.1         - 8.8.8.8 
- hosts: "{{ target_hosts | default('aws') }}"   become: yes   become_user: root   gather_facts: false    tasks:     - name: Install python       raw: dnf install python -y     - name: install packages       package:          name: "{{ item }}"         state: present       loop:         - git         - docker         - make         - ansible         - grubby     - name: install packages       pip:         name: "{{ item }}"       loop:         - docker         - docker-compose         - ansible-tower-cli     - name: cater for cgroup v2       shell: grubby --update-kernel=ALL --args="systemd.unified_cgroup_hierarchy=0"     - name: reboot for kernel params to take effect       reboot:     - name: wait for machine to come up       wait_for_connection:         connect_timeout: 20         sleep: 5         delay: 5         timeout: 300     - name: pull AWX from git repo       git:         repo: https://github.com/ansible/awx         dest: /awx         force: yes         version: 3.0.2     - name: Start docker daemon       service:         name: docker         enabled: yes         state: started     - name: Change value of host key checking on AWX server       lineinfile:         path: /etc/ansible/ansible.cfg         regexp: '#host_key_checking = False'         line: 'host_key_checking = False'      - name: generate ssh key for local use       command: ssh-keygen -b 2048 -t rsa -f /root/.ssh/id_rsa -q -N ""       args:         creates: /root/.ssh/id_rsa     - name: configure local ssh access       copy:          remote_src: True          src: /root/.ssh/id_rsa.pub         dest: /root/.ssh/authorized_keys     - name: Add local key to authorized_keys       authorized_key:         user: root         state: present         key: "{{ lookup('file', '/root/.ssh/id_rsa.pub') }}"     - name: change value of host in installer inventory       lineinfile:         path: /awx/installer/inventory         regexp: 'localhost ansible_connection=local ansible_python_interpreter="/usr/bin/env python"'         line: 'localhost'     - name: change value of pg_port in installer inventory       lineinfile:         path: /awx/installer/inventory         regexp: 'pg_port=5432'         line: 'pg_port="5432"'     - name: install AWX       command:  ansible-playbook -i /awx/installer/inventory /awx/installer/install.yml      - name: Configure tower-cli       shell: "{{ item }}"        with_items:         - tower-cli config verify_ssl false         - tower-cli config username admin         - tower-cli config password password         - tower-cli config host http://127.0.0.1     - name: Check to ensure AWX has come up successfully - this can take some time.       shell: tower-cli project list       register: result       until: result.stdout.find("Demo Project") != -1       retries: 10       delay: 30     - name: copy workflow schema file to node       copy:         src: files/pacman_workflow.yml         dest: /root/pacman_workflow.yml         mode: 0755      - name: copy workflow telemetry streaming schema file to node       copy:         src: files/ts_workflow.yml         dest: /root/ts_workflow.yml         mode: 0755      - name: copy workflow deploy as3 schema file to node       copy:         src: files/deploy_as3.yml         dest: /root/deploy_as3.yml         mode: 0755      - name: copy tower config file to node       copy:         src: files/setup-tower.sh         dest: /root/setup-tower.sh         mode: 0755      - name: Run tower setup script       shell: /root/setup-tower.sh     - name: Configure tower-cli       shell: "{{ item }}"        with_items:         - tower-cli workflow create --name "webapp_workflow"       #  - tower-cli workflow schema webapp_workflow @/root/pacman_workflow.yml         - tower-cli project create --name ansible-AS3 --scm-type git --scm-url http://github.com/codecowboydotio/ansible-AS3 --organization Default --wait         - tower-cli inventory create --name aws-bigip --organization Default         - tower-cli inventory_source create --name "aws-source" --inventory "aws-bigip" --source ec2 --credential "aws-account" --update-on-launch "true" --overwrite "true" --instance-filters "tag:boxtype=bigip"         - tower-cli job_template create --name "Update AS3 declaration" --job-type run --inventory aws-bigip --project ansible-AS3 --playbook AS3-select-vip.yml --credential root-ssh --extra-vars "target_hosts=all aws_region=ap-southeast-2"         - tower-cli job_template associate_credential --credential aws-account --job-template "Update AS3 declaration"         - tower-cli job_template create --name "Post AS3 declaration" --job-type run --inventory aws-bigip --project ansible-AS3 --playbook AS3.yml --credential root-ssh --extra-vars "target_hosts=all bigip_port=8443 HTTP_METHOD=POST file_name=AS3-vip.json"         - tower-cli job_template associate_credential --credential aws-account --job-template "Post AS3 declaration"         - tower-cli workflow create --name "deploy_as3"         - tower-cli workflow schema deploy_as3 @/root/deploy_as3.yml         - tower-cli job_template create --name "create_bigip" --job-type run --inventory localhost --project ansible --playbook bigip_aws_awx.yml --ask-variables-on-launch "true" --extra-vars "target_hosts=all security_group=sg-000 subnet_id=subnet-000 keypair=keypair bigip_password=admin bigip_license=AAA"         - tower-cli job_template associate_credential --credential aws-account --job-template "create_bigip"         - tower-cli job_template create --name "install telegraf" --job-type run --inventory AWS --project ansible --playbook telegraf.yml --ask-variables-on-launch "true" --extra-vars "target_hosts=all beacon_access_token=AAA"         - tower-cli job_template associate_credential --credential root-ssh --job-template "install telegraf"         - tower-cli workflow schema webapp_workflow @/root/pacman_workflow.yml     - name: Install F5 SDK       shell: docker exec awx_task pip install f5-sdk 
--- - name: Ansible Lockdown Remediation Role   hosts: all   become: true   roles:       - role: "{{ playbook_dir }}" 
#!/usr/bin/env ansible-playbook ---  - import_playbook: asserts.yml  - name: configure backpninja for nodes   hosts: backupninja   gather_facts: False    tasks:      - delegate_to: localhost       block:         - { make: { chdir: ., file: dups.mk, target: main }, run_once: True }         - { find: { path: 'tmp/{{ inventory_hostname }}/', patterns: '*.dup' }, register: find }     - copy:         src: '{{ item.path }}'         dest: '/etc/backup.d'         mode: 0600       become: True       loop: '{{ find.files }}'       loop_control: { label: '{{ item.path }}' }  
#!/usr/local/bin/ansible-playbook --inventory=inventory.yaml - name: '{{ ansible_name }} | coredns.yml'    hosts: cloudctl   vars_files:     - 'vars/global.yml'     - 'vars/run.yml'     - '../cluster-vars.yml'   vars:     module: "coredns"     ansible_name_module: "{{ ansible_name }} | {{ module }}"     listen_address: "{{ lookup('env', 'PUBLISH_ADDRESS') | default(ansible_default_ipv4.address, true) }}"   roles:     - role: coredns       when: start_dns == true 
--- # playbook for creating and configuring new users - hosts: '{{ target }}'   user: '{{ admin_user }}'   become: yes    vars_files:     - vars/user.yml    vars:     homebrew_apps:       - git     cask_apps:       - adobe-creative-cloud       - colorpicker-skalacolor       - github       - google-chrome       - harvest       - kaleidoscope       - microsoft-office       - qlcolorcode       - qlimagesize       - qlmarkdown       - qlstephen       - quicklook-json       - sketch       - slack       - transmit       - visual-studio-code     node_global_packages:       - browser-sync       - gulp-cli       - npm    pre_tasks:     - name: Stay awake for 20 minutes       shell: caffeinate -t 900 &      - name: Check if user exists       stat: path=/Users/{{ username }}       register: check_user      - set_fact: user_home=/Users/{{ username }}       tags: always     - set_fact: user_lib=/Users/{{ username }}/Library       tags: always     - set_fact: user_lib_prefs={{ user_lib }}/Preferences       tags: always      - name: Get available account pictures       shell: ls -1 files/user_account_images/*.png       register: account_images       ignore_errors: yes       become: no       connection: local      - name: Default to an empty list if no images are found       # The `with_random_choice` command needs a list to pull from, it fails loudly if the list is empty       # Sending a list with one empty string allows us to use `account_images.stdout_lines[0]` as a boolean       set_fact:          account_images:             stdout_lines: ['']       when: account_images.stdout_lines | length == 0      - name: Send new account picture       copy:         src: '{{ item }}'         dest: '/Library/User Pictures/Ansible/{{ item | basename }}'         mode: 0644       when: account_images.stdout_lines[0]       with_random_choice: '{{ account_images.stdout_lines }}'       ignore_errors: yes       register: account_image       become: yes      - name: Handle missing images so we can pass an empty string to the role       set_fact:         account_image: { results: [ { item: '' } ] }       when: account_image.results[0].item is not defined    roles:     - global-prefs     - { role: new-user, image: "{{ account_image.results[0].item | basename}}", when: not check_user.stat.exists }     - { role: terminal, tags: [terminal] }     - { role: homebrew, apps: "{{ homebrew_apps }}", become: no }     - { role: homebrew_cask, apps: "{{ cask_apps }}", become: no }     - { role: node.js, become: no }     - { role: safari, when: user_lib is defined and user_lib_prefs is defined }     - { role: finder, when: user_lib is defined and user_lib_prefs is defined }     - { role: dock, when: user_lib is defined and user_lib_prefs is defined }     # - { role: vagrant, vagrant_user: "{{ username }}", tags: [vagrant] }     - { role: mail, tags: [mail] }     - { role: sketchapp }     - { role: vscode }     - { role: git }    tasks:     - set_fact:         font_dirs:           - "{{ user_lib }}"           - "{{ user_lib }}/Fonts"      - name: Relax permissions on user's Library/Fonts directory       file:         path: "{{ item }}"         mode: 0777         state: directory       with_items: '{{ font_dirs }}'      - name: Install default font set from fonts/       synchronize:         src: fonts/         dest: "{{ user_lib }}/Fonts/"         rsync_opts: ["--exclude=.gitkeep"]         archive: no         recursive: yes      - name: Set user as owner of all fonts in Library/Fonts       file:         path: "{{ font_dirs | last }}"         owner: "{{ username }}"         group: staff         mode: 0755         state: directory         recurse: yes      - name: Restore permissions on user's Library/Fonts directory       file:         path: "{{ item }}"         mode: 0700         state: directory       with_items: '{{ font_dirs }}'   # Second playbook for running the local template stuff - hosts: '{{ target }}'   connection: local   become: no    vars_files:     - vars/user.yml    tasks:     - name: Generate Welcome message on controller       template:          src: '{{ item }}'         dest: ./Welcome-{{ username }}.html       tags: ['welcome', 'template']       with_first_found:         - templates/welcome.html.j2         - templates/welcome_sample.html.j2      - name: Open generated HTML template       shell: open ./Welcome-{{ username }}.html -a Safari       tags: ['welcome', 'template'] 
---  - name: install python to run ansible playbook   hosts: ubuntu   become: true   gather_facts: False   tasks:   - name: install python 2.7 on ubuntu     raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal)   - name: make apt frontend noninteractive     raw: export DEBIAN_FRONTEND=noninteractive  - name: Include a play after another play   import_playbook: webservers.yml   tags: webservers 
- name: Proxmox configuration   hosts: ogd:&hypervisor   become: true   gather_facts: false   pre_tasks:     - name: End play for hypervisors that are detected falsely       ansible.builtin.meta: end_host       when: hypervisor is not defined   roles:     - role: lae.proxmox       vars:         # Would remove Proxmox completely.         # https://github.com/lae/ansible-role-proxmox/issues/223         pve_remove_old_kernels: false       tags: proxmox     - role: proxmox       tags: proxmox     - role: smartmontools       tags: smartmontools  - name: Import Linux configuration playbook   tags: linux   ansible.builtin.import_playbook: _linux.yml   vars:     playbook_hosts: ogd:&linux  - name: Import Fedora configuration playbook   tags: fedora   ansible.builtin.import_playbook: _fedora.yml   vars:     playbook_hosts: ogd:&fedora  - name: Gateway configuration   hosts: gateway-ogd   become: true   gather_facts: false   roles:     - role: ssh       tags: ssh     - role: gateway       tags: gateway  - name: Hosted services   hosts: finn   become: true   gather_facts: false   roles:     - role: smartmontools       tags: smartmontools     - role: docker       tags: docker     - role: dns-server       tags:         - dns-server         - container     - role: mqtt       tags:         - mqtt         - container     - role: traefik       tags:         - traefik         - container     - role: iobroker       tags:         - iobroker         - container     - role: logger       tags:         - logger         - container     - role: samba       tags: samba 
# To Run this Playbook Issue the command # ansible-playbook nmap.yml # Author Anish Nath --- - hosts: localhost   become: yes   gather_facts: false   roles:     - { role: nmap } 
- name: Raul's homeserver Ansible playbook   hosts: all   become: true    roles:     - { role: samba }     - { role: docker }     - { role: deploy_containers }     - { role: bagsplus_backup }     - { role: setup_backups }     - { role: fb_group_scraper } 
#!/usr/bin/env ansible-playbook --- - hosts: backend   tasks:     - name: include version vars       include_vars:         file: cli-vars.yml      - name: Install Site Reliability Engineering (SRE) tools       pip:         name: [locust, powerfulseal]         state: present         executable: pip3       become: yes      - name: Install node and requested node packages       import_role:         name: nodejs       vars:         role_spec:           version: '{{ node_version }}'           packages: '{{ node_packages }}'      - name: Clone ros projects       import_role:         name: git       vars:         role_spec:           project_dir: "{{ lookup('pipe', 'pwd') }}/.."           repositories:             ros:               url: https://github.com/rails-on-services/cnfs-cli.git               name: cli               version: "{{ ros_cli_version }}"       when: clone_repos|bool == True      - name: Configure cli       import_role:         name: cli       vars:         project_dir: "{{ lookup('pipe', 'pwd') }}/../cli/cli"       when: clone_repos|bool == True 
# Ansible Playbook for Subutai WordPress blueprint ---  - hosts: drupal   remote_user: root   tasks:      - name: Disable dpkg fsync       raw: test -e /etc/dpkg/dpkg.cfg.d/unsafe-io || echo force-unsafe-io > /etc/dpkg/dpkg.cfg.d/unsafe-io      - name: Upgrade debian       apt:         update_cache: true         upgrade: true      - name: Install required debs       apt:         name: "{{ item }}"         state: present       with_items:          - php7.0         - php7.0-cgi         - php7.0-cli         - php7.0-mysql         - php7.0-xml         - php7.0-mbstring         - php7.0-gd         - php7.0-curl         - apache2         - libapache2-mod-php7.0         - unzip          - curl         - mariadb-server         - python-mysqldb      - name: Install limits.ini       template:          src: files/limits.ini         dest: /etc/php/7.0/mods-available/limits.ini      - name: Activate limits.ini       shell: phpenmod limits      - name: Create database       mysql_db:         name: drupal7         state: present         encoding: utf8      - name: Create database user       mysql_user:         name: drupal7         password: drupal7         priv: "drupal7.*:ALL"      - name: Get drush       get_url:         url: https://github.com/drush-ops/drush/releases/download/8.1.18/drush.phar         dest: /usr/local/bin/drush         validate_certs: no      - name: Make drush executable       file:         path: /usr/local/bin/drush         state: touch         mode: u+rwx,g+rx,o+rx      - name: Download Drupal7 core       shell: drush dl drupal-7 --destination=/var/www  --drupal-project-rename=drupal -y       args:         chdir: /var/www         creates: /var/www/drupal/index.php      - name: Site Install with drush       shell: drush si standard --account-name="{{ admin_user }}" --account-mail="{{ email }}" --account-pass="{{ admin_pwd }}" --site-name="{{ site_title }}" --db-url=mysql://drupal7:drupal7@localhost/drupal7 -y       args:          chdir: /var/www/drupal         creates: /var/www/drupal/sites/default/settings.php      - name: Remove original index.html       file:         name: /var/www/drupal/index.html         state: absent      - name: Change base_url on drupal settings file       become: true       lineinfile:         dest: /var/www/drupal/sites/default/settings.php         regexp: "# $base_url = 'http://www.example.com'; "         line: "$base_url = 'https://{{ domain_name }}';"         state: present      - name: Add https conf       become: true       lineinfile:         dest: /var/www/drupal/sites/default/settings.php         line: "$conf['https'] = TRUE;"         state: present      - name: Change ownership       file:          path: /var/www/drupal         recurse: yes         owner: www-data         group: www-data      - name: Setup default site on apache       template:          src: files/000-default.conf         dest: /etc/apache2/sites-available/000-default.conf         force: yes      - name: Enable mod_rewrite       apache2_module:         name: rewrite         state: present      - name: Restart apache       service:         name: apache2         state: restarted 
#!/usr/bin/env ansible-playbook --- - hosts: pladd-laptop    tasks:   - name: disable_sshd.service     service:       name: sshd.service       state: stopped       enabled: false      - name: enable_sshd.socket     service:       name: sshd.socket       state: started       enabled: true    - name: ssh_config     blockinfile:       path: /home/pladd/.ssh/config       create: yes       owner: pladd       group: pladd       mode: u=rw,go=-       block: |         Host github.com           HostName github.com           User laddp           IdentityFile ~/.ssh/id_rsa         ### from https://source.redhat.com/groups/public/itmarketing/it_marketing_wiki/ssh_bastion_config_2018_edition         Host *            Compression                 yes           ControlMaster               auto           #ControlPath                ~/.cache/ssh/%C.sock # Only enable this if you understand what it does           ControlPersist              4h           ServerAliveInterval         300           ServerAliveCountMax         600           GSSAPIDelegateCredentials   yes           GSSAPIAuthentication        yes           ForwardAgent                no                                                                  ForwardX11                  no                                                                     ForwardX11Trusted           no            Include                     /etc/crypto-policies/back-ends/openssh.config  ... 
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.cron     - role: robertdebock.update     - role: robertdebock.cis       # 1.1.6 Ensure separate partition exists for /var (Scored)       # Vagrant image has no /var.       cis_var_partition: no       # 1.1.7 Ensure separate partition exists for /var/tmp (Scored)       # Vagrant image has no /var/tmp.       cis_var_tmp_partition: no       # 1.1.11 Ensure separate partition exists for /var/log (Scored)       # Vagrant image has no /var/log.       cis_var_log_partition: no       # 1.1.12 Ensure separate partition exists for /var/log/audit (Scored)       # Vagrant image has no /var/log/audit.       cis_var_log_audit_partition: no       # 1.1.13 Ensure separate partition exists for /home (Scored)       # Vagrant image has no /home.       cis_home_partition: no 
--- - name: "Creating Kubernetes High Availability Cluster with Ansible Playbook"   hosts: etcd master node   roles:     - { role: 'init', tags: "init",when: "ansible_os_family == 'RedHat'" }  - name: "签发证书"   hosts: localhost   connection: local   roles:     - { role: 'cert', tags: "cert" }  - name: "配置etcd"   hosts: etcd   roles:     - { role: 'etcd', tags: "install_etcd" }  - name: "配置master"   hosts: master   roles:     - { role: 'flannel', tags: "install_master" }     - { role: 'master', tags: "install_master" }  - name: "安装kubectl"   hosts: kubectl   roles:     - { role: 'kubectl', tags: "install_kubectl" }  - name: "配置node"   hosts: node   roles:     - { role: 'flannel', tags: "install_node" }     - { role: 'docker', tags: "install_node" }     - { role: 'node', tags: "install_node" }  
--- author:   name: Alexander Krantz   links:     - type: email       email: alex@krantz.dev     - type: phone       phone: +1 778 873 1160     - type: website       url: https://krantz.dev     - type: github       username: akrantz01     - type: linkedin       username: akrantz01  education:   - id: bcit-cst     school: British Columbia Institute of Technology     degree: Computer Systems Technology     date:       start: 2024-01-08       end: 2026-04-30     location: Vancouver, BC, Canada     gpa: 93%     courses:       - Web Development I & II       - Programming Methods       - Business Analysis and System Design    - id: ubc-ee     school: University of British Columbia     degree: Bachelor of Applied Science     area: Electrical Engineering     date:       start: 2020-09-08       end: 2023-12-23     location: Vancouver, BC, Canada     courses:       - Introduction to Microcomputers       - Data Structures and Algorithms       - Symbolic Logic     extra: >-       #text(weight: "medium")[Progress]: unfinished  experience:   - id: sai-summer     company: Sanctuary AI     title: Cognitive Platform Co-op     date:       start: 2024-06-01       end: 2024-08-30     location: Vancouver, BC, Canada     highlights:       - >-         Campaigned across software and embedded teams to promote Ansible adoption and knowledge,         leading to a 75% increase in team member confidence and ~25 new playbooks for automating         processes.       - >-         Reduced deployment time from 1 day to 15 minutes by applying best practices to Ansible         playbooks and roles.       - >-         Improved internal Ansible developer experience by introducing CI/CD pipelines to run         automated linting, role testing in ephemeral Docker containers, playbook integration         testing on dedicated test hardware, and inventory validation.    - id: sai-fall     company: Sanctuary AI     title: Systems Integration Co-op     date:       start: 2023-09-01       end: 2023-12-31     location: Vancouver, BC, Canada     highlights:       - >-         Declaratively created a collection of continuous deployment pipelines on AWS to deploy two         services across multiple accounts, ensuring the separation of environments using AWS CDK         and CodePipelines.       - >-         Removed all usages of static AWS credentials in favor of assume role credentials to increase         security.       - Reduced the size of a service's container by 85% and sped up its startup time by 100 times.    - id: shopify-payments     company: Shopify     title: Payments Infrastructure, Backend Developer Intern     date:       start: 2023-05-01       end: 2023-08-31     location: Vancouver, BC, Canada     highlights:       - >-         Made a parameter optional across the service to reduce data duplication and streamline         usage for clients.       - >-         Assisted in preparing for Black Friday/Cyber Monday traffic by performing load tests and         validating Grafana alerts.       - >-         Prevented potential fines from NACHA by handling account errors in ACH return transaction         processing.    - id: shuttle     company: Shuttle     title: Open Source Contractor     date:       start: 2022-10-01       end: 2023-02-28     location: Vancouver, BC, Canada     highlights:       - >-         Discovered a vulnerability that allowed remote code execution and full database access via         SQL injection.       - >-         Designed and implemented a system that issues TLS certificates for custom domains using         LetsEncrypt.    - id: shopify-fulfillment     company: Shopify     title: Shopify Fulfillment Network, Backend Developer Intern     date:       start: 2022-05-01       end: 2022-08-31     location: Redwood City, CA, USA     highlights:       - >-         Minimized shipping time for packages by selecting carriers based on historical performance         data.       - Improved error handling for parcel carriers not supporting tracking created labels.       - >-         Added strict typing to the codebase using Sorbet to improve developer experience and         prevent errors.    - id: influxdata     company: InfluxData     title: Software Engineering Intern     date:       start: 2021-05-01       end: 2021-09-30     location: Redwood City, CA, USA     highlights:       - >-         Implemented ~25 bug fixes and features in Telegraf's database, cloud service, and         orchestration interactions.       - >-         Interacted with community developers to troubleshoot and resolve issues and pull requests         on GitHub.  volunteering:   - id: wafflehacks     company: WaffleHacks     title: Directory of Technology     date:       start: 2021-02-01       end: 2024-09-01     url: https://wafflehacks.org     highlights:       - >-         Developed and deployed a custom application portal using Python and TypeScript that was         visited by ~3500 participants over 4 events.       - >-         Leveraged distributed tracing via OpenTelemetry to detect reliability issues and reduce         latency by 250ms.       - >-         Coordinated a team of student developers to build a judging platform, Discord bot, and         website.  projects:   - id: antsi     title: antsi     description: A quick and easy way to style text for the terminal     date:       start: 2024-03-17       end: 2024-04-18     url:       type: github       href: https://github.com/akrantz01/antsi     details:       - >-         Provides a simply Python API wrapping a Rust core through the PyO3 library to minimize the         performance impact.       - >-         Built a custom markup language parser to assign decorations to text, minimizing the number         of escape sequences.    - id: lers     title: lers     description: An asynchronous, user-friendly ACME client for Rust     date:       start: 2023-03-14       end: 2023-03-30     url:       type: github       href: https://github.com/akrantz01/lers     details:       - Implements the full RFC 8555 specification, including external account bindings.       - >-         Provides robust, bundled challenge solvers for the HTTP-01, DNS-01, and TLS-ALPN-01         challenges.    - id: davoxide     title: DAVOxide     description: A simple WebDAV server with a minimal web UI     date:       start: 2022-07-21       end: 2022-08-13     url:       type: github       href: https://github.com/akrantz01/davoxide     details:       - A Rust-based class 1 WebDAV server implementing all required features of RFC 4918.       - >-         Implemented read-only access to files through a web UI using GraphQL written in TypeScript         using React.       - >-         Built a directory- and file-level permissions system using proxy authentication and         database-backed authorization.    - id: lights     title: Lights     description: An internet-controlled LED strip     date:       start: 2021-09-30       end: 2022-04-03     url:       type: github       href: https://github.com/akrantz01/lights     details:       - >-         Drives a strip of Adafruit NeoPixels with a Rasperry Pi 4 through a Rust-based library for         GPIO control.       - >-         Provides real-time control with a web interface using WebSockets and Redux for client-side         state synchronization.       - >-         Implemented customizable animations using WebAssembly modules loaded and executed         on-demand.  awards:   - id: treehacks-cognito     name: Best Startup by YCombinator     organization: TreeHacks 2023     on: 2023-02-19     url:       text: Devpost       href: https://devpost.com/software/cognito-uf3rs1     description: >-       Developed a web-based tool called Cognito with a friend that uses AI combined with natural       language queries to provide companies insight into their customers, audiences, potential       leads using publicly available data.    - id: frontierhacks-atomicnet     name: 1st Place     organization: FrontierHacks     on: 2019-11-24     url:       text: Devpost       href: https://devpost.com/software/atomicnet     description: >-       Developed a hardware and software solution called AtomicNet to prevent internet outages in a       team of 4. It uses low-cost, low-power Rasperry Pis to act as an alternative or more       resilient internet, ensuring that internet outages are minimized. The software component is a       suite of applications that proide basic chat, forums, and information sharing functionality.    - id: bsa-eagle     name: Eagle Scout     organization: Boy Scouts of America     on: 2019-01-08  skills:   "Programming Languages":     - Python     - Rust     - Go     - TypeScript     - Ruby     - SQL     - C     - Java   "Frameworks & Libraries":     - React     - Ruby on Rails     - Next.js     - Svelte     - OpenTelemetry     - OAuth2     - gRPC   "Tools & Technologies":     - Amazon Web Serivces     - Git     - Google Cloud Platform     - Docker     - Kubernetes     - Linux     - PostgreSQL     - Redis     - Terraform     - Ansible     - GitHub Actions     - GitLab CI 
# Ansible playbook comprimises of hosts and task which in turns # contains task name and module name  --- # start of index - host: webserver   become: yes   tasks:   - name: Installing httpd      yum:       name: httpd       state: latest   - name: Starting the httpd     service:       name: httpd       status: started  # ansible-playbook -i inv web.yml --check 
--- - name: Configure gpg   hosts: workstation   tasks:    - name: Ensure required software packages to run this playbook are installed     ansible.builtin.dnf:       name:         - pcsc-lite         - pcsc-tools         - python3-psutil     become: true    - name: Start and enable pcscd.service     ansible.builtin.systemd:       name: pcscd.service       state: started       enabled: true     become: true    - name: Start and enable pcscd.socket     ansible.builtin.systemd:       name: pcscd.socket       state: started       enabled: true     become: true    - name: Create a directory for '~/.gnupg'     ansible.builtin.file:       path: $HOME/.gnupg       state: directory       mode: '0700'     become: false    - name: Create a '~/.gnupg/gpg.conf' file     ansible.builtin.copy:       src: gpg.conf       dest: $HOME/.gnupg/gpg.conf       force: true       mode: '0600'     become: false    - name: Setup gpg key     ansible.builtin.command: >-       bash -c "         if ! gpg --list-key stefan.widgren@gmail.com; then           gpg --import files/key.asc           KEYID=$(gpg --list-keys --with-colons stefan.widgren@gmail.com | awk -F: '/fpr:/ {print $10}' | head -n 1)           echo -e '5\ny\n' | gpg --command-fd 0 --edit-key $KEYID trust           echo 'Setup gpg'         fi"     register: setup_gpg     changed_when: "'Setup gpg' in setup_gpg.stdout"     become: false    - name: Create a configuration file for gpg-agent     ansible.builtin.copy:       src: gpg-agent.conf       dest: $HOME/.gnupg/gpg-agent.conf       force: false     become: false    - name: Ensure the 'SSH_AUTH_SOCK' is set     ansible.builtin.lineinfile:       path: $HOME/.bashrc       regexp: '^export SSH_AUTH_SOCK='       line: export SSH_AUTH_SOCK=$(gpgconf --list-dirs agent-ssh-socket)     become: false    - name: Ensure the 'gpg-agent' is launched     ansible.builtin.lineinfile:       path: $HOME/.bashrc       regexp: '^gpgconf --launch gpg-agent'       line: gpgconf --launch gpg-agent     become: false    - name: Create a '~/.ssh' directory     ansible.builtin.file:       path: $HOME/.ssh       state: directory       mode: '0700'     become: false    - name: Create a configuration file for ssh     ansible.builtin.copy:       src: ssh-config       dest: $HOME/.ssh/config       force: false       mode: '0600'     become: false    - name: Setup ssh key     ansible.builtin.command: >-       bash -c "         if test ! -f ${HOME}/.ssh/id_rsa_yubikey.pub; then           gpg --export-ssh-key stefan.widgren@gmail.com > $HOME/.ssh/id_rsa_yubikey.pub           chmod 600 $HOME/.ssh/id_rsa_yubikey.pub           echo 'Setup ssh'         fi"     register: setup_ssh     changed_when: "'Setup ssh' in setup_ssh.stdout"     become: false    - name: Ensure the 'alias switch-yubikey' is set     ansible.builtin.lineinfile:       path: $HOME/.bashrc       regexp: '^alias switch-yubikey='       line: alias switch-yubikey="gpg-connect-agent 'scd serialno' 'learn --force' /bye"     become: false 
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.epel     - role: robertdebock.npm 
# code: language=ansible # Ansible Playbook for AryaOS # # Copyright Sensors & Signals LLC https://www.snstac.com # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. #  --- - name: Default   hosts: all   become: true   vars_files:     - vars.yml   pre_tasks:     - name: Update repository index (Debian)       tags: always       ansible.builtin.apt:         cache_valid_time: 86400       changed_when: false       when: ansible_distribution == "Ubuntu" or ansible_distribution == "Debian"   roles:     - role: stage-base     - role: stage-aryaos     - role: stage-pytak     - role: stage-node-red     - role: stage-docker     - role: stage-adsbcot     - role: stage-aiscot     - role: stage-dronecot 
--- # Run using the following: # ansible-playbook --ask-sudo-pass yarn.yml - hosts: 127.0.0.1   connection: local   become_user: root   vars:     - apt_file: /etc/apt/sources.list.d/yarn.list   tasks:     - name: Add apt key       apt_key:         url: https://dl.yarnpkg.com/debian/pubkey.gpg         state: present       become: yes      - name: Update apt repository       apt_repository:         repo: deb https://dl.yarnpkg.com/debian/ stable main         state: present         update_cache: yes       become: yes      - name: Install Yarn       apt: pkg=yarn state=present       become: yes
---   # Continue Run with Run Id:     ansible-playbook tb.yml -e "exp=demo run=1611332286"  - name: Run tensorboard   hosts: leonhard   tasks:   - assert:       that:       - run != 'new'    - name: Load an Experiment Run (init a new run if it does not exist yet)     include_role:       name: experiment-state     vars:       expstate: load    - name: If there are unfinished jobs left, load them     include_role:       name: lsf-monitor-jobs     vars:       lsf_monitor_check_wait_time: 0 # Override wait time       force_refresh: "{{ force | default(false) }}"     when: lsf_job_ids_unfinished | length > 0 # there are some unfinished jobs left    - name: Run tensorboard     delegate_to: localhost     command:       cmd: "tensorboard --logdir={{ experiment_results_dir }}/{{ exp }}_{{ run }}/"   # TODO: Integrate tensorboard into monitoring loop 
---  - ansible.builtin.import_playbook: dbservers.yml  - ansible.builtin.import_playbook: webservers.yml 
#!/usr/bin/env ansible-playbook ---  - hosts: all   become: true    vars_files:     - custom.yml     - secret.yml    handlers:     - import_tasks: handlers/main.yml    pre_tasks:     - set_fact:         aws: "{{ lookup('ansible.builtin.env', 'AWS_EC2') == 'true' }}"         letsencrypt_staging: "{{ [lookup('ansible.builtin.env', 'LETSENCRYPT_STAGING') | lower | default('no')] }}"    roles:     - role: system       tags:         - system      - role: docker       tags:         - docker      - role: fail2ban       when: enable_fail2ban | default(False)       tags:         - fail2ban      - role: ufw       when: enable_ufw | default(False)       tags:         - ufw      - role: dns       when: enable_adguard_unbound_doh | default(False)       tags:         - dns      - role: authelia       tags:         - authelia      - role: bunkerweb       tags:         - bunkerweb      - role: wireguard       tags:         - wireguard      - role: chriswayg.msmtp-mailer       when: email_password is defined       tags:         - msmtp      - role: ssh       tags:         - ssh 
--- # This Ansible playbook is just a test of setting up various corners of a web # dev box automatically.  - name: Do everything   # TODO Doesn't *really* apply to all hosts - just a dev setup.   hosts: all   # TODO Figure out how to handle permissions sanely.   sudo: yes   roles:     - avahi 
#The Ansible Playbook above is responsible for configuring the EC2 instance and copy over the files/index.js file.  ---  # Exercise for setting up a web server in an EC  - name: Exercise for setting up a web server in an EC     # hosts: ubuntu    # Use the public IP if the VM is already created    hosts: all    user: ubuntu    become: true    become_method: sudo    become_user: root    roles:     - apache     - prepare
#!/usr/bin/env ansible-playbook ---  - hosts: local   gather_facts: False    roles:     - role: .       accounts:         - id: &id '{{ user }}'           host: '{{ host }}'           user: *id           maildir: '{{ maildir }}'           patterns: '{{ patterns }}' 
#!/usr/local/bin/ansible-playbook --inventory=inventory.yaml - name: '{{ ansible_name }} | tree.yml'    hosts: cloudctl   become_user: root   become: true   vars_files:     - 'vars/global.yml'   vars:     module: "tree"     ansible_name_module: "{{ ansible_name }} | {{ module }}"   tasks:      - name: '{{ ansible_name_module }} | file | Create Directory Structure'        file:         path: '{{ item }}'         state: directory         mode: '0755'       with_items:         - '{{ dir_platform }}/config/user-data'         - '{{ dir_platform }}/cluster/manifests'         - '{{ dir_platform }}/cluster/ignition'         - '{{ dir_platform }}/cluster/aux'         - '{{ dir_platform }}/cluster/aux/manifests'         - '{{ dir_platform }}/cluster/aux/openshift'         - '{{ dir_platform }}/cluster/config'         - '{{ dir_platform }}/cluster/'         - '{{ dir_platform }}/mirror'         - '{{ dir_platform }}/images'         - '{{ dir_platform }}/nginx'         - '{{ dir_platform }}/iac'     - file:         path: '{{ item }}'         state: directory         mode: '0644'       with_items:         - '{{ dir_secrets }}/registry/ssl'         - '{{ dir_secrets }}/registry/auth'         - '{{ dir_secrets }}/cluster'         - '{{ dir_secrets }}/docker'         - '{{ dir_secrets }}/ssh'         - '{{ dir_local_home }}/.kube'         - '{{ dir_tmp }}' 
--- # compute-video-demo-ansible - name: Include GCE Instance Create playbook   import_playbook: gce-instances.yml - name: Include Run Code playbook   import_playbook: runcode.yml - name: Include GCE Cleanup playbook   import_playbook: cleanup.yml 
--- # Ansible playbook for an ELK (Elasticsearch, Logstash, Kibana) logging server. # # original @author Jeff Geerling (2014). - hosts: elk   sudo: true    vars_files:     - vars/main.yml    vars:     logstash_main_server_common_name: "elk"     logstash_ssl_dir: /etc/pki/logstash     logstash_ssl_certificate_file: lumberjack.crt     logstash_ssl_key_file: lumberjack.key    roles:     #- { role : bastly.openssl }     - bastly.logstash     - bastly.java     - bastly.nginx     - bastly.elasticsearch     - bastly.elasticsearch-curator     - bastly.kibana 
---  ################################################################## # An Ansible playbook to configure UFW firewall on Ubuntu 20.04. # ##################################################################  # Defining target section. - name: UFW configuraton    hosts: all   become: true   gather_facts: true  # Defining task section.   tasks:   - name: Configuring UFW     ansible.builtin.debug:       msg: '[INFO] CONFIGURING UFW...'      - name: Deny incoming     ansible.builtinufw:       direction: incoming       proto: any       policy: deny     notify:     - reload ufw    - name: Allow outgoing     ansible.builtinufw:       direction: outgoing       proto: any       policy: allow     notify:     - reload ufw    - name: Allow SSH     ansible.builtinufw:       rule: allow       name: OpenSSH     notify:     - reload ufw    - name: Enable service     ansible.builtinufw:       state: enabled    - name: End status     ansible.builtindebug:       msg: '[OK] Finished, firewall configured.'  # Defining handler section.   handlers:   - name: reload ufw     ansible.builtinufw:       state: reloaded    - name: End playbook     ansible.builtin.meta: end_play 
--- # one playbook, three plays # # run # ansible-playbook play.yml # # This file is a simple playbook.  # The playbook contains three plays.  # Each play contains one task.  # # For more complex examples, try these files next.  # task-line-order.yml - a task is a YAML dictionary, and rearranging its lines makes no difference.  # directive-handler.yml - examples of special handler tasks #  # - name: play 1   hosts: localhost    gather_facts: no   become: no   connection: local   vars:     playbook_var: playbook_val    tasks:    - name: task in play 1     debug:        msg: This is play 1    - name: play 2   hosts: localhost    gather_facts: no   become: no   connection: local    tasks:    - name: task in play 2     debug:        msg: This is play 2    - name: play 3   hosts: localhost    gather_facts: no   become: no   connection: local    tasks:    - name: task in play 3     debug:        msg: This is play 3  
# linode galera ansible playbook  # cluster pre-check - name: pre-check   hosts: galera_servers   user: root   vars_files:     - group_vars/galera/vars     - group_vars/galera/secret_vars   roles:     - precheck  # linodes - name: configure galera linodes   hosts: galera_servers   any_errors_fatal: true   user: root   vars:     local_home: "{{ lookup('env','HOME') }}"   vars_files:     - group_vars/galera/vars     - group_vars/galera/secret_vars      roles:     - common     - db     - post
--- - name: Ansible playbook for configuring my Mac workstation   hosts: localhost   roles:     - role: ansible-git       when: configure_git     - role: ansible-hosts       when: configure_hosts     - role: ansible-shell-mac       when: configure_shell     - role: ansible-yubikey       when: configure_yubikey     - role: ansible-ssh       when: configure_ssh     - role: ansible-osx_defaults       when: configure_osx     - role: ansible-homebrew       when: configure_brew     - role: ansible-vscode       when: configure_vscode     - role: ansible-mas       when: configure_mas 
#!/usr/bin/env ansible-playbook --- - name: Install and configure   hosts: all   become: true    vars_files:     - custom.yml     - secret.yml    handlers:     - name: Import global handlers       ansible.builtin.import_tasks: handlers/main.yml    pre_tasks:     - name: Set the AWS and Letsencrypt staging facts       ansible.builtin.set_fact:         aws: "{{ lookup('ansible.builtin.env', 'AWS_EC2') == 'true' }}"         letsencrypt_staging: "{{ [lookup('ansible.builtin.env', 'LETSENCRYPT_STAGING') | lower | default('no')] }}"    roles:     - role: system       tags:         - system      - role: geerlingguy.docker       tags:         - docker      - role: docker       tags:         - docker      - role: fail2ban       when: enable_fail2ban | default(False)       tags:         - fail2ban      - role: dns       when: enable_adguard_unbound_doh | default(False)       tags:         - dns      - role: watchtower       tags:         - watchtower      - role: authelia       tags:         - authelia      - role: web       tags:         - web      - role: wireguard       tags:         - wireguard      - role: ssh       tags:         - ssh 
--- # Copyright (c) 2021 Luca Berton # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the "Software"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in all # copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE.  rh_bullettin: >-   https://access.redhat.com/security/vulnerabilities/RHSB-2021-009 intro: |   Ansible Playbook tested with detector version 1.3 released 2022-01-10.   If a 404 error occur please adjust the URL with the latest version available   for detector URL.   Please refer to the Red Hat Security Bullettin for up-to-date information and   adjust the playbook variables accordingly.   {{ rh_bullettin }}. vulnerable: |   System MIGHT be vulnerable to log4j (CVE-2021-44228) not_vulnerable: |   System IS NOT vulnerable to log4j (CVE-2021-44228) report_txt: "/report/vuln_log4j2_path_*.txt" sh_detector: "cve-2021-44228--2022-01-10-1242.sh" sh_signature: "cve-2021-44228--2022-01-10-1242.sh.asc" detector_baseurl: "https://access.redhat.com/sites/default/files/" force_download: false detector_path: "/var/" detector_dir: "/opt/cve-2021-44228/" detector_run_dir: "tmp" detector_options: '-n -d --no-progress --scan {{ detector_path }}' gpg_keyid: "7514F77D8366B0D9" gpg_server: "pgp.mit.edu" gpg_public_key: 'gpg --keyserver {{ gpg_server }} --recv {{ gpg_keyid }}' clean_run_before: true delete_after: false verify_gpg: true 
--- - name: Windows Playbook   hosts: all    vars_files:     - default.config.yml    pre_tasks:     - name: Include playbook user configuration.       ansible.builtin.include_vars: "{{ item }}"       with_fileglob:         - "{{ playbook_dir }}/config.yml"       tags: ["always"]    tasks:     - name: Include hostname configuration.       ansible.builtin.import_tasks:         file: tasks/hostname.yml       when: configure_hostname       tags: ["hostname"]      - name: Include windows updates configuration.       ansible.builtin.import_tasks:         file: tasks/updates.yml       when: install_windows_updates       tags: ["updates"]      - name: Include debloat configuration.       ansible.builtin.import_tasks:         file: tasks/debloat.yml       when: remove_bloatware       tags: ["debloat"]      - name: Include chocolatey configuration.       ansible.builtin.import_tasks:         file: tasks/chocolatey.yml       when: install_chocolatey_packages       tags: ["choco"]      - name: Include winget configuration.       ansible.builtin.import_tasks:         file: tasks/winget.yml       when: install_winget_packages       tags: ["winget"]      - name: Include windows features configuration.       ansible.builtin.import_tasks:         file: tasks/windows_features.yml       when: install_windows_features       tags: ["windows_features"]      - name: Include wsl2 configuration.       ansible.builtin.import_tasks:         file: tasks/wsl2.yml       when: install_wsl2       tags: ["wsl"]      - name: Include fonts configuration.       ansible.builtin.import_tasks:         file: tasks/fonts.yml       when: install_fonts       tags: ["fonts"]      - name: Include ohmyposh configuration.       ansible.builtin.import_tasks:         file: tasks/ohmyposh.yml       when: install_ohmyposh       tags: ["ohmyposh"]      - name: Include explorer configuration.       ansible.builtin.import_tasks:         file: tasks/explorer.yml       when: configure_explorer       tags: ["explorer"]      - name: Include taskbar configuration.       ansible.builtin.import_tasks:         file: tasks/taskbar.yml       when: configure_taskbar       tags: ["taskbar"]      - name: Include start menu configuration.       ansible.builtin.import_tasks:         file: tasks/start_menu.yml       when: configure_start_menu       tags: ["start_menu"]      - name: Include sounds configuration.       ansible.builtin.import_tasks:         file: tasks/sounds.yml       when: set_sound_scheme       tags: ["sounds"]      - name: Include mouse configuration.       ansible.builtin.import_tasks:         file: tasks/mouse.yml       when: disable_mouse_acceleration       tags: ["mouse"]      - name: Include power plan configuration.       ansible.builtin.import_tasks:         file: tasks/power_plan.yml       when: change_power_plan       tags: ["power"]      - name: Include remote desktop configuration.       ansible.builtin.import_tasks:         file: tasks/remote_desktop.yml       when: remote_desktop_enabled       tags: ["remote_desktop"]      - name: Include desktop configuration.       ansible.builtin.import_tasks:         file: tasks/desktop.yml       when: remove_desktop_icons       tags: ["desktop"]      - name: Include storage sense configuration.       ansible.builtin.import_tasks:         file: tasks/storage.yml       when: configure_storage_sense       tags: ["storage_sense"]      - name: Include defrag configuration.       ansible.builtin.import_tasks:         file: tasks/defrag.yml       when: defrag_volumes       tags: ["defrag"] 
- name: configuring radon_master   hosts: master   gather_facts: true   sudo: yes   roles:     - radon_master  - hosts: monitor   tags:     - grafana   roles:     - { role: grafana, grafana_exec_vars_only: true }   tasks:     - name: Ensure grafana API Key directory exists       file:         path: "{{ playbook_dir }}/conf/keys"         state: directory       delegate_to: localhost      - name: Check grafana API Key list       uri:         url: "http://{{ ansible_host }}:{{ grafana_port }}/api/auth/keys"         user: "{{ grafana_admin_user }}"         password: "{{ grafana_admin_password }}"         force_basic_auth: yes         return_content: yes       register: existing_api_keys      - name: Check grafana API Key file existed       stat:         path: "{{ playbook_dir }}/conf/keys/grafana_apikey.key"       register:  grafana_apikey_file       delegate_to: localhost      - set_fact:         apikey_id: "{{ item }}"       with_items: "{{ existing_api_keys.json|json_query(apikey_id_query) }}"       vars:         apikey_id_query: "[?name=='grafana_apikey'].id"       when:         - ((existing_api_keys['json'] | selectattr("name", "equalto", "grafana_apikey")) | list) | length == 1         - grafana_apikey_file.stat.exists == False      - debug:         var: apikey_id       when:         - ((existing_api_keys['json'] | selectattr("name", "equalto", "grafana_apikey")) | list) | length == 1         - grafana_apikey_file.stat.exists == False      - name: Delete grafana API Key when grafana API Key file is missing       uri:         url: "http://{{ ansible_host }}:{{ grafana_port }}/api/auth/keys/{{ apikey_id }}"         user: "{{ grafana_admin_user }}"         password: "{{ grafana_admin_password }}"         force_basic_auth: yes         method: DELETE       when:         - ((existing_api_keys['json'] | selectattr("name", "equalto", "grafana_apikey")) | list) | length == 1         - grafana_apikey_file.stat.exists == False      - name: Create grafana API Key       uri:         url: "http://{{ ansible_host }}:{{ grafana_port }}/api/auth/keys"         user: "{{ grafana_admin_user }}"         password: "{{ grafana_admin_password }}"         force_basic_auth: yes         method: POST         body_format: json         body: "{{ item | to_json }}"       with_items: "{{ grafana_api_keys }}"       when: (((existing_api_keys['json'] | selectattr("name", "equalto", item['name'])) | list) | length == 0) or (((existing_api_keys['json'] | selectattr("name", "equalto", "grafana_apikey")) | list) | length == 1 and grafana_apikey_file.stat.exists == False)       register: new_api_keys      - name: Create grafana API key file       become: no       copy:         dest: "{{ playbook_dir }}/conf/keys/{{ item['item']['name'] }}.key"         content: "{{ item['json']['key'] }}"         backup: no       when: item['json'] is defined       with_items: "{{ new_api_keys['results'] }}"       delegate_to: localhost      - name: import grafana data source       shell: >         chdir={{ grafana_dir }}/data         warn=no         curl -q -X POST -d @data_source.json --header 'Content-Type: application/json'         "http://{{ grafana_admin_user }}:{{ grafana_admin_password }}@127.0.0.1:{{ grafana_port }}/api/datasources"      - name: import grafana dashboards - prepare config       delegate_to: localhost       template: src=grafana.dest.json.j2 dest={{ playbook_dir }}/scripts/dests.json       vars:         - ansible_become: false         - ansible_connection: local         - grafana_dest_config:             name: "{{ cluster_name | title }}"             url: "http://{{ansible_default_ipv4.address}}:{{ grafana_port }}/"             user: "{{ grafana_admin_user }}"             password: "{{ grafana_admin_password }}"             apikey: "{{ lookup('file', '{{ playbook_dir }}/conf/keys/grafana_apikey.key') }}"             datasource: "{{ cluster_name }}"             titles:               node: "{{ cluster_name | title }}-Node"               mysql: "{{cluster_name | title}}-Mysql"               radon: "{{cluster_name | title}}-Radon"      - name: import grafana dashboards - run import script       delegate_to: localhost       shell: >-         chdir={{ playbook_dir }}/scripts         python grafana-config-copy.py       vars:         - ansible_become: false         - ansible_connection: local 
#!/usr/bin/ansible-playbook --inventory=inventory - name: ' Sparta | Devkit'   hosts: local   vars:     module: 'devkit'     ansible_name_module: ' Sparta | Devkit'     sparta_path: '{{ local_home }}/Sparta/Git'   vars_files:     - 'vars/git.yml'  - import_playbook: ./code/tree.yml         # Create base deploy directory tree - import_playbook: ./code/git.yml          # Pull git repos 
---  # http://serverfault.com/questions/638507/how-to-access-host-variable-of-a-different-host-with-ansible # We need some facts from the install node, specifically to find address of slurm controller. - name: grab facts from production nodes   hosts: production   tasks: [ ]  - name: NFS Node   hosts: nfs   remote_user: root   become: yes    roles:   - { role: watermark-release, tags: [ 'always' ]  }   - { role: network_interface, tags: [ 'network' ] }   - { role: ansible-role-hosts-int, tags: [ 'hosts'] }   - { role: ansible-role-fgci-repo, tags: [ 'fgci7', 'repos' ] }   - { role: ansible-role-yum, tags: [ 'yum', 'repos', 'login' ] }   - { role: ansible-role-ferm-firewall, tags: [ 'firewall', 'network' ] }   - { role: dns, tags: [ 'dns' ] }   - { role: ansible-role-chrony, tags: [ 'ntp', 'chrony' ] }   - { role: ansible-role-yum-cron-2, tags: [ 'yumcron' ] }   - { role: ansible-role-rsyslog, tags: [ 'rsyslog' ] }   - { role: ansible-role-nis, tags: [ 'nis' ] }   - { role: ansible-role-aliases, tags: [ 'aliases', 'email' ] }   - { role: ansible-role-smartd, tags: [ 'smartd' ] }   - { role: ansible-role-idmapd, tags: [ 'idmapd' ] }   - { role: ansible-role-nfs, tags: [ 'nfs' ] } #  - { role: ansible-role-cvmfs, tags: [ 'cvmfs' ] }   - { role: ansible-role-rdma, tags: [ 'rdma', 'infiniband' ] }   - { role: ansible-role-dell, tags: [ 'dell' ] } #  - { role: ansible-role-collectd, tags: [ 'collectd', 'monitoring' ] }   - { role: ansible-role-sshd, tags: [ 'sshd', 'ssh' ] }   - { role: ansible-role-postfix, tags: [ 'postfix', 'mail' ] }   - { role: ansible-role-systemd-journal, tags: [ 'systemd', 'journal', 'journald' ] }  - name: Mount /home and /scratch on Install Node   hosts: install   become: yes    roles:   - { role: ansible-role-nfs_mount, tags: [ 'nfsmount' ] }   - { role: ansible-role-sshd-host-keys, tags: [ 'host-keys' ] }   # Local addons to this playbook # - import_playbook: nfs_local_play.yml 
--- - name: "Main Ansible playbook"   hosts: localhost   connection: local      vars_files:     - ./vars/vault_vars.yml     - ./vars/vars.yml    vars_prompt:     - name: nas_pw       prompt: "Insert NAS user {{ nas_user }} password"    pre_tasks:   - name: "Update Cache"     become: yes     apt:       update_cache: yes       cache_valid_time: 3600    tasks:    - include: ./tasks/packages.yml   - include: ./tasks/dotfiles.yml   - include: ./tasks/neovim.yml   - include: ./tasks/keepassxc.yml   - include: ./tasks/rclone.yml   - include: ./tasks/veracrypt.yml   - include: ./tasks/firefox.yml   - include: ./tasks/chrome.yml   - include: ./tasks/brave.yml   - include: ./tasks/spotify.yml   - include: ./tasks/discord.yml   - include: ./tasks/nas.yml   - include: ./tasks/python.yml   - include: ./tasks/llvm.yml   - include: ./tasks/icarus.yml   - include: ./tasks/verilator.yml   - include: ./tasks/riscv.yml 
--- # ansible-playbook main.yml -i hosts --tags docker_apps -K # the binary | this.file | -i targets | *optional --tags | -K asks for sudo pass  - hosts: all   sudo: true    roles:     - docker     - mergerfs     - snapraid     - epsilon 
--- # EticaAI/aguia-pescadora-ansible-playbooks/tldr.yml  ### 'git clone' falhou? ________________________________________________________ # Você precisa instalar o Git https://git-scm.com  ### 'ansible-playbook' falhou (não está instalado)? ____________________________ # Você precisa instalar ele para rodar este arquivo. https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html  ### Você tsuru client na sua máquina local? ____________________________________ ### Você docker na sua máquina local? __________________________________________ ### Você docker-machine na sua máquina local? __________________________________ # As checagens a seguir fazem isso antes mesmo do usuário seguir adiante. Porém # nos NÃO instalamos na máquina local para o usuário. Como ansible não está # ignorando erros, o usuário irá ver bem claro a falha e o link direto para # documentação da instalação  - name: tldr tsuru client? docker? docker-machine?   hosts: localhost   tasks:     - name: tsuru client? https://tsuru-client.readthedocs.io/en/latest/installing.html       shell: tsuru --version       ignore_errors: false      - name: docker? https://docs.docker.com/install/       shell: docker --version       ignore_errors: false      - name: docker-machine? https://docs.docker.com/machine/install-machine/       shell: docker-machine version       ignore_errors: false  - name: Hack para permitir o não uso de ansible inventory e forçar -e paas_host=example.com como host alvo   hosts: localhost   connection: local   tasks:     - name: modo para forçar servidor de destino como target       add_host:         name: "{{ paas_host }}"         groups: paas_host       changed_when: false  - name: Checagem se você tem acesso via SSH ao paas_host definido (se falhar, você PRECISA que o ansible pelo menos consiga acessar via SSH!)   hosts: paas_host   gather_facts: true   tasks:     - name: Tem acesso?       debug:         msg: "Tesde de acesso ao {{ ansible_host }}"  - name: "Prepara o Host que vai conter o Tsuru (Instala o OpenResty com AutoSSL e o Minio)"   hosts: paas_host   remote_user: root   roles:     - common     - minio     - paas-host  - name: "Usando Tsuru Client desta máquina local, instala o Tsuru server na máquina destino"   hosts: localhost   vars:     paas_master_ip: "{{ paas_ip }}"     paas_node1_ip: "{{ paas_ip }}"     paas_cluster_name: "{{ paas_host }}"   roles:     - tsuru-installer  
---  - hosts: all    tasks:     - name: ansibleplaybook-simple       debug:         msg: Your are running 'ansibleplaybook-simple' example 
# file: dns.yaml # # Creates security enabled DNS zones via the Managed DNS provider API. # # ansible-playbook -vvv dns.yaml -i zones # ---  - name: MANAGED DNS   hosts: localhost   connection: local   roles:   - dynect_zones  - name: MANUAL STEPS   hosts: localhost   connection: local   gather_facts: no   tasks:   - name: DNSSEC instructions     debug: msg="Remember to add the Delegation Signer (DS) records                 from the DNS management console to your domain registrations." 
--- # Default Molecule .env.yml file # NOTE: to make molecule use this file, place it in the root of your project (or from whichever directory you are calling 'molecule'), and rename it to .env.yml PLAYBOOK_DIR: ../../../playbooks/ # Relative to the default molecule.yml (molecule/ext/molecule-src/molecule.yml) ANSIBLE_VERBOSITY: '2' ANSIBLE_ROLES_PATH: ../../playbooks/roles # Relative to your scenario (e.g. molecule/role-foo/..) ANSIBLE_STDOUT_CALLBACK: yaml REQUIREMENTS_FILE: requirements.txt 
# Ansible playbook to install Java 7 on Debian # # Thanks to http://www.sysadminslife.com/linux/howto-oracle-sun-java-7-installation-unter-debian-6-squeeze/ # # Author: Gabriel Birke <gb@birke-software.de>   - name: Install wget     yum: pkg=wget    - name: Download Java     command: 'wget -q -O {{java_archive}} --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F" {{java_download}} creates={{java_archive}}'     - name: Extract Java archive     command: "tar -xzf {{java_archive}} -C {{download_folder}} creates={{download_folder}}/{{java_name}}"    - name: Fix ownership      file: "state=directory path={{download_folder}}/{{java_name}} owner=root group=root recurse=yes"    - name: Create Java folder     file: "state=directory path={{java_folder}}"    - name: Move Java to final folder     command: "mv {{download_folder}}/{{java_name}} {{java_folder}} creates={{java_folder}}/{{java_name}}"    - name: Create symlink for easier update     file: "state=link src={{java_folder}}/{{java_name}} dest={{java_folder}}/{{java_alias}}"    - name: Make Java available for system     command: 'update-alternatives --install "/usr/bin/java" "java" "{{java_folder}}/{{java_alias}}/bin/java" 1' 
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.epel     - role: robertdebock.python_pip     - role: robertdebock.ara 
apiVersion: v1 kind: Pod metadata:   name: ansible spec:     containers:     - name: ansible       image: quay.io/philips/ansible:4       command: ["/usr/local/bin/ansible-playbook", "-c", "chroot", "-i", "/etc/ansible/hosts", "/etc/ansible/site.yml"]        # TODO: Create a wrapper that will take check and diff as an environment variable or something       #command: ["/usr/local/bin/ansible-playbook", "-c", "chroot", "-i", "/etc/ansible/hosts", "/etc/ansible/site.yml", "--check", "--diff"]        securityContext:         runAsUser: 0         privileged: true       volumeMounts:       - mountPath: /media/root         name: root       - mountPath: /etc/ansible         name: site       - mountPath: /tmp         name: tmp     volumes:     - name: root       hostPath:         path: /         type: Directory     - name: tmp       emptyDir: {}     - name: site       projected:         sources:         - configMap:             name: ansible             items:               - key: site.yml                 path: site.yml         - configMap:             name: ansible             items:               - key: ansible.cfg                 path: ansible.cfg         - configMap:             name: ansible             items:               - key: hosts                 path: hosts 
#!/usr/bin/env ansible-playbook --- - name: move_to_collections   hosts: localhost   become: false   gather_facts: true   gather_subset:     - min    vars_files:     - vars/main.yml    tasks:     - name: set role_path and role_name       set_fact:         role_path: "{{ lookup('env', 'PWD') }}"         role_name: "{{ lookup('env', 'PWD') | basename | regex_replace('ansible-role-') }}"      - name: move_to_collections       shell: "set -o pipefail ; ../move_to_collections/transform.sh"       register: transformed       changed_when: false       failed_when: false 
scripts:   encrypt_prod_env: |     ansible-vault encrypt .env.prod   decrypt_prod_env: |     ansible-vault decrypt .env.prod   ansible: |     cd ansible && ansible-playbook main.yaml   connect-ansible-hosts: |     cd ansible && ansible-playbook playbooks/connect_hosts.yaml   restart-pi-services: |     cd ansible && ansible-playbook playbooks/restart_services.yaml
version: 1.0 name: galera-apb description: Deploy Galera for MySQL with a Replication Operator bindable: True async: optional metadata:   displayName: Galera Cluster for MySQL   documentationUrl: "https://github.com/beekhof/galera-container/"   longDescription: "An Ansible Playbook Bundle (APB) that deploys a Galera cluster for MySQL managed by a replication operator" tags:   - database   - galera plans:   - name: ephemeral     description: This plan deploys an ephemeral Galera cluster on Linux     free: True     metadata: {}     parameters:       - name: admin_user         title: Openshift Admin User         type: string         default: developer         required: true       - name: admin_password         title: Openshift Admin Password         type: string         display_type: password         default: password         required: true       - name: service_name         description: The name of the service. Used to name and label resources         type: string         default: galera         pattern: "^[a-zA-Z0-9]+[a-zA-Z0-9-]*[a-zA-Z0-9]+$"         required: true       - name: service_replicas         title: Number of database replicas         type: number         default: 3         required: true       - name: mysql_database         description: The name of the MySQL database         type: string         default: sample         pattern: "^[a-zA-Z0-9_]*[a-zA-Z_]+[a-zA-Z0-9_]*$"         required: true       - name: mysql_user         description: Username that will be used to connect to MySQL         type: string         pattern: "^[a-zA-Z0-9_]*[a-zA-Z_]+[a-zA-Z0-9_]*$"         default: admin         required: true       - name: mysql_password         description: Password to connect to MySQL (generated if blank)         type: string         required: true         display_type: password         default: admin   - name: persistent     description: This plan deploys a persistent Galera cluster on Linux     free: True     metadata: {}     parameters:       - name: admin_user         title: Openshift Admin User         type: string         default: developer         #required: true       - name: admin_password         title: Openshift Admin Password         type: string         display_type: password         default: password       - name: service_name         description: The name of the service. Used to name and label resources         type: string         default: galera         pattern: "^[a-zA-Z0-9]+[a-zA-Z0-9-]*[a-zA-Z0-9]+$"         required: true       - name: service_replicas         title: Number of database replicas         type: number         default: 3       - name: mysql_database         description: The name of the MySQL database         type: string         default: sample         pattern: "^[a-zA-Z0-9_]*[a-zA-Z_]+[a-zA-Z0-9_]*$"         required: true       - name: mysql_user         description: Username that will be used to connect to MySQL         type: string         pattern: "^[a-zA-Z0-9_]*[a-zA-Z_]+[a-zA-Z0-9_]*$"         default: admin         required: true       - name: mysql_password         description: Password to connect to MySQL (generated if blank)         type: string         required: true         display_type: password         default: admin       - name: volume_size         type: enum         default: '1Gi'         enum: ['1Gi', '5Gi', '10Gi']         title: MySQL Volume Size         required: true 
#!/usr/local/bin/ansible-playbook --inventory=inventory.yaml - name: '{{ ansible_name }} | ssh.yml'    hosts: cloudctl   become_user: root   become: true   vars_files:     - 'vars/global.yml'   vars:     module: "ssh"     ansible_name_module: "{{ ansible_name }} | {{ module }}"   roles:     - ssh 
version: 1.0 name: mssql-remote-apb description: Microsoft SQL Server Remote APB  bindable: True async: optional metadata:   displayName: "MS SQL Server - Remote"   imageUrl: https://raw.githubusercontent.com/VeerMuchandi/Using-OpenShift-APB/master/mssql-rhel/SQL-Server.png   longDescription: "An Ansible Playbook Bundle (APB) that deploys a database on an existing MS SQL Server configured for contained databases"   providerDisplayName: "Red Hat, Inc."  plans:   - name: default     description: This plan deploys a database on your own instance of MS SQLServer. Use this APB when you want to provision a database on your own database server.  You will supply connection information to your database in the next step. Your MS SQLServer should have been configured to support contained databases as explained here https://www.mssqltips.com/sqlservertip/2428/sql-server-2012-contained-database-feature/     free: True     metadata:        displayName: MS SQL Server       longDescription: Provide your server connection information, and the database and user to add. Refer Password Policies https://docs.microsoft.com/en-us/sql/relational-databases/security/password-policy#password-complexity     parameters:       - name: mssql_database         title: Name of the database to be provisioned          type: string         default: sampledb         pattern: "^[a-zA-Z_][a-zA-Z0-9_]*$"         required: true       - name: mssql_host         title: Host running MS SQL Server          type: string         required: true       - name: mssql_port         title: Port MS SQL Server is exposed on         type: int          default: 1433          required: true       - name: mssql_admin_user         title: MSSQL Admin User         type: string         default: SA          required: true         maxlength: 128       - name: mssql_admin_password         title: MSSQL Admin Password         type: string         required: true         display_type: password         maxlength: 128         pattern: "^[a-zA-Z0-9_~!@#$%^&*()-=<>,.?;:|]+$"       - name: mssql_user         title: Database user to be added to connect in the future          type: string         default: mssqluser         required: true         maxlength: 128       - name: mssql_password         title: Password for the database user being added         default: MSSQLPassword1         display_type: password         description: "As per SQL Server password policy, the password must be atleast 8 characters long and contain characters from three of the following four sets: Uppercase letters, Lowercase letters, Base 10 digits, and Symbols."         type: string         required: true         minlength: 8            maxlength: 128         pattern: "^[a-zA-Z0-9_~!@#$%^&*()-=<>,.?;:|]+$"       - name: postinstall_mssql_script         title: URL of the script to run after database creation (optional)         type: string         required: false 
--- - git: https://github.com/osism/cfg-generics.git   version: main   files:     - src: src/render-ansible-requirements.py       dst: ./environments/manager/render-ansible-requirements.py     - src: gilt.yml       dst: ./gilt.yml     - src: requirements.txt       dst: ./requirements.txt     - src: environments/manager/images.yml       dst: environments/manager/images.yml     - src: environments/manager/run.sh       dst: environments/manager/run.sh     - src: environments/manager/ansible.cfg       dst: environments/manager/ansible.cfg     - src: environments/manager/playbook-*.yml       dst: environments/manager/     - src: environments/manager/requirements.*       dst: environments/manager/       post_commands:         - python3 render-ansible-requirements.py         - rm render-ansible-requirements.py 
--- # ansible-playbook main.yml -i hosts --tags docker_apps -K # the binary | this.file | -i targets | *optional --tags | -K asks for sudo pass  - hosts: all   sudo: true   roles:     - arch-postinstall 
#!/usr/bin/env ansible-playbook --- - hosts: pladd-znc   vars_files:   - passwords.vault    tasks:   - name: fail if vault not decrypted     fail: msg="Vault not decrypted"     when: pladd_znc_dyndns is undefined    - name: disable_sshd.service     service:       name: sshd.service       state: stopped       enabled: false      - name: enable_sshd.socket     service:       name: sshd.socket       state: started       enabled: true    - name: DDNS-key     lineinfile:       path: /etc/redhat-internal-ddns/hosts       line: "pladd-znc usersys.redhat.com {{ pladd_znc_dyndns }}"       create: true    - import_tasks: epel-task.yml   - import_tasks: rh_certs_tasks.yml    - name: packages     package:       name:       - redhat-internal-cert-install       - redhat-internal-backup-config       - redhat-internal-ddns-client       - firewalld       - znc    - name: services     service:       name: firewalld       state: started       enabled: true    - name: firewalld_znc     firewalld:       port: "{{ item }}/tcp"       state: enabled       permanent: yes       immediate: yes     with_items:     - 1025     - 6667    - name: firewalld_ssh     firewalld:       service: ssh       state: enabled       permanent: yes       immediate: yes    - name: znc_config     copy:       src: znc.conf       dest: /var/lib/znc/.znc/configs/       decrypt: yes       owner: znc       group: znc       mode: "u=rw"    - name: services     service:       name: znc       state: started       enabled: true    - name: ddns_enable     command: redhat-internal-ddns-client.sh enable  ... 
--- ## cat roles/ui/README.md # ansible-playbook -i inventories/... -K --tags "update_service_files" ui.yaml # ansible-playbook -i inventories/... -K --tags "restart_services" ui.yaml  - name: Deploy UI   hosts: ui   sudo: yes   roles:       - role: notify-chat         username: UI         icon: ":cinema:"         msg: "Deploying UI"        - role: ui        - role: notify-chat         username: UI         icon: ":cinema:"         msg: ":heavy_check_mark: Done deploying UI"  
#!/usr/local/bin/ansible-playbook --inventory=inventory - name: '{{ ansible_name }} | bundle | build'    hosts: containerone   vars_files:     - '../p1.yml'     - 'vars/{{ target_environment }}.yml'     - 'vars/global.yml'   vars:     module: "build"     state_provider: "local"     terraform_module_path: "{{ dir_git_tfcore }}"    #terraform_global_variables: "{{ dir_terraform }}/p1.tfvars"     ansible_name_module: "{{ ansible_name }} | {{ module }}"   tasks:      ####### Terraform Apply     - name: '{{ ansible_name_module }} | terraform | Build VPC'       terraform:         state: present         project_path: "{{ terraform_module_path }}"        #variables_file: "{{ terraform_global_variables }}"       register: terraform_output 
#Ansible Playbook for Users   #This will run the script Ansible-playbook Users.yml Vars: #Variables      http_port: 80     max_clients: 200  -name: creating a demo user # Task   os_user:  # Module     name: demouser     password: secret-pass     update_password: on_create     email: demo@example.com     domain: "{{ demo_domain.id }}"     state: present  -name: adding role to the group   os_user_role: # Module     group: demo2     role: demorole     domain: "{{ demo_domain.id }}"  -name: ensure apache is at the latest version   yum:     name: httpd     state: latest  handlers: # Handler      - name: restart memcached       service:         name: memcached         state: restarted      - name: restart apache       service:         name: apache         state: restarted                                                                                                                     
-Guide d'administration système avec Ansible  Ansible Tutorial for Beginners  -Documentation sur l'inventaire Ansible  Documentation Ansible Inventory  -Documentation de la collection Community.Docker pour Ansible  Community Docker Collection  -Meilleures pratiques pour les Playbooks Ansible  Ansible Playbook Best Practices   --- - hosts: webservers   become: true   become_method: sudo   vars:     docker_volume: db_data     db_name: db    tasks:     - name: Install required system packages       apt:         pkg:           - apt-transport-https           - ca-certificates           - curl           - software-properties-common           - python3-pip           - virtualenv           - python3-setuptools         state: latest         update_cache: true      - name: Add Docker GPG apt Key       apt_key:         url: https://download.docker.com/linux/ubuntu/gpg         state: present      - name: Add Docker Repository       apt_repository:         repo: deb https://download.docker.com/linux/ubuntu focal stable         state: present      - name: Update apt and install docker-ce       apt:         name: docker-ce         state: latest         update_cache: true      - name: Stop and remove Apache container if it exists       community.docker.docker_container:         name: webapp         state: absent      - name: Create Apache container       community.docker.docker_container:         name: webapp         image: httpd         ports:           - "8080:80"         state: present      - name: Copy index.html to webservers       copy:         src: files/index.html         dest: "/tmp/index.html"      - name: Copy index.php to webservers       copy:         src: files/index.php         dest: "/tmp/index.php"      - name: Copy httpd.conf to webservers       copy:         src: templates/httpd.conf         dest: "/tmp/httpd.conf"      - name: Check if webapp container is running       community.docker.docker_container_info:         name: webapp       register: webapp_info       ignore_errors: true      - debug:         var: webapp_info      - name: Copy index.html to container       community.docker.docker_container_copy_into:         container: webapp         path: /tmp/index.html         container_path: /usr/local/apache2/htdocs/index.html       when: webapp_info is defined and webapp_info.container.State.Running | default(false)      - name: Copy index.php to container       community.docker.docker_container_copy_into:         container: webapp         path: /tmp/index.php         container_path: /usr/local/apache2/htdocs/index.php       when: webapp_info is defined and webapp_info.container.State.Running | default(false)      - name: Copy httpd.conf to container       community.docker.docker_container_copy_into:         container: webapp         path: /tmp/httpd.conf         container_path: /usr/local/apache2/conf/httpd.conf       when: webapp_info is defined and webapp_info.container.State.Running | default(false)      - name: Set ServerName to suppress warnings       community.docker.docker_container_exec:         container: webapp         command: bash -c 'echo "ServerName localhost" >> /usr/local/apache2/conf/httpd.conf'       when: webapp_info is defined and webapp_info.container.State.Running | default(false)      - name: Restart Apache in the container       community.docker.docker_container_exec:         container: webapp         command: service apache2 restart       when: webapp_info is defined and webapp_info.container.State.Running | default(false)      - name: Launch database container       community.docker.docker_container:         name: "{{ db_name }}"         image: mysql:5.7         ports:           - "3307:3306"         volumes:           - "{{ docker_volume }}:/var/lib/mysql:rw"         restart: true         env:           MYSQL_ROOT_PASSWORD: tarik           MYSQL_DATABASE: db           MYSQL_USER: tarik           MYSQL_PASSWORD: tarik 
#!/usr/bin/env ansible-playbook --- - hosts: all   become: true   roles:     - patch 
#!/usr/bin/env ansible-playbook --- #==============================================================# # File      :   etcd.yml # Desc      :   init etcd on special group etcd # Ctime     :   2022-11-10 # Mtime     :   2023-09-30 # Path      :   etcd.yml # Author    :   Ruohang Feng (rh@vonng.com) # License   :   AGPLv3 #==============================================================# - name: ETCD INIT   hosts: etcd   gather_facts: no   become: yes   roles:     - { role: node_id , tags: id  }     - { role: etcd }  #--------------------------------------------------------------- # Usage #--------------------------------------------------------------- # 1. make sure etcd is defined # # etcd: #   hosts: #     10.10.10.10: { etcd_seq: 1 } #     10.10.10.11: { etcd_seq: 2 } #     10.10.10.12: { etcd_seq: 3 } #   vars: #     etcd_cluster: etcd # # 2. Run playbook: ./etcd.yml -l etcd # #--------------------------------------------------------------- # Tasks #--------------------------------------------------------------- # etcd_assert     : generate minio identity # etcd_install    : install etcd rpm packages # etcd_clean      : cleanup existing etcd #   - etcd_check  : check etcd instance is running #   - etcd_purge  : remove running etcd instance & data # etcd_dir        : create etcd data & conf dir # etcd_config     : generate etcd config #   - etcd_conf   : generate etcd main config #   - etcd_cert   : generate etcd ssl cert # etcd_launch     : launch etcd service # etcd_register   : register etcd to prometheus #--------------------------------------------------------------- ...
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.common     - role: robertdebock.epel     - role: robertdebock.python_pip       python_pip_modules:         - name: ansible     - role: robertdebock.docker     - role: robertdebock.awx 
---  - name: Initialize WSL Environment   hosts: localhost   gather_facts: true   tasks:     - name: Fail if not executed by root       ansible.builtin.command: whoami       changed_when: false       register: whoami       failed_when: '"root" not in whoami.stdout'      - name: Create Main User Role       ansible.builtin.include_role:         name: create_main_user       vars:         v_create_main_user_admin_group: "{{ admin_group }}"         v_create_main_user_username: "{{ dev_username }}"         v_create_main_user_rootdir: "{{ root_folder_for_workspace }}"      - name: Add to known hosts to avoid RSA issues later       ansible.builtin.include_role:         name: add_known_hosts       vars:         v_add_known_hosts_host_list: "{{ ssh_known_hosts }}"         v_add_known_hosts_dev_username: "{{ dev_username }}"      - name: Create Main Development Directories       ansible.builtin.file:         path: "{{ item }}"         state: directory         mode: '0755'         owner: "{{ dev_username }}"         group: "{{ dev_username }}"       loop:         - "{{ root_folder_for_dev }}"         - "{{ root_folder_for_workspace }}"         - "/home/{{ dev_username }}/.local"         - "/home/{{ dev_username }}/.local/bin"      - name: Ensure local bin in profile for path variable       ansible.builtin.lineinfile:         dest: "/home/{{ dev_username }}/.profile"         state: present         regexp: '^%export PATH=/home/{{ dev_username }}/.local/bin'         line: 'export PATH=/home/{{ dev_username }}/.local/bin:$PATH'         insertbefore: BOF      - name: Add Test Playbook       ansible.builtin.include_role:         name: add_test_playbook       vars:         v_add_test_playbook_rootdir: "{{ root_folder_for_repo }}"         v_add_test_playbook_username: "{{ dev_username }}"      - name: Add Ansible Lint Config       ansible.builtin.include_role:         name: create_ansiblelint_config       vars:         v_create_ansiblelint_config_rootdir: "{{ root_folder_for_ansible_lint_config }}"         v_create_ansiblelint_config_full_filename: "{{ ansible_lint_config_file }}"         v_create_ansiblelint_config_username: "{{ dev_username }}"      - name: Setup WSL Conf Settings       ansible.builtin.include_role:         name: create_wsl_conf       vars:         v_create_wsl_conf_username: "{{ dev_username }}"         v_create_wsl_conf_rootdir: "{{ root_folder_for_dev }}"      - name: Install Some Main Packages       ansible.builtin.include_role:         name: install_main_packages       vars:         v_install_main_packages_install_list: "{{ system_packages }}"      - name: Install Python3-pip and Python Packages       ansible.builtin.include_role:         name: install_python_packages       vars:         v_install_python_packages_install_list: "{{ python_modernized_packages }}"      - name: Install Ansible Collections - root       ansible.builtin.include_role:         name: install_ansible_collections       vars:         v_install_ansible_collections_collection_list: "{{ ansible_collection_install_list }}"      - name: Install podman for Ubuntu       ansible.builtin.include_role:         name: install_podman_for_ubuntu       vars:         v_install_podman_for_ubuntu_username: "{{ dev_username }}"      - name: Install Ansible Navigator       ansible.builtin.include_role:         name: install_ansible_navigator       vars:         v_install_ansible_navigator_username: "{{ dev_username }}"         v_install_ansible_navigator_setting_container_engine: "podman"    # Would require coding to include docker         v_install_ansible_navigator_setting_execution_image: "{{ ansible_navigator_default_eei }}"         v_install_ansible_navigator_setting_pull_policy: "{{ ansible_navigator_pull_policy }}"         v_install_ansible_navigator_pods_to_pull: "{{ ansible_pods_to_pull }}"         v_install_ansible_navigator_rootdir: "{{ root_folder_for_dev }}"      - name: Install packer       ansible.builtin.include_role:         name: install_packer       vars:         v_install_packer_apt_repo: "{{ ubuntu_os_product_name }} main"      - name: Install terraform       ansible.builtin.include_role:         name: install_terraform       vars:         v_install_terraform_apt_repo: "{{ ubuntu_os_product_name }} main"      - name: Install Sentinel       ansible.builtin.include_role:         name: install_sentinel       vars:         v_install_sentinel_username: "{{ dev_username }}"         v_install_sentinel_download_url: "{{ sentinel_download_url }}"      - name: Install NVM and NPMs with version manager       ansible.builtin.include_role:         name: install_npm_with_version_manager       vars:         v_install_npm_with_version_manager_username: "{{ dev_username }}"         v_install_npm_with_version_manager_package_list: "{{ npm_packages }}"      - name: Get Starting Git Repos       ansible.builtin.git:         repo: "{{ item.repo_https }}"         version: "{{ item.version }}"         dest: "{{ item.dest }}"       environment:         GIT_TERMINAL_PROMPT: "0"    # reports "terminal prompts disabled" on missing password       when: item.dest != root_folder_for_dev + "/wsl_image_setup"    # dont try to update what we are running       loop: "{{ git_repo_to_add }}"       become_user: "{{ dev_username }}"       become: true      - name: Add Scheduled Tasks       ansible.builtin.include_role:         name: add_scheduled_tasks       vars:         v_add_scheduled_tasks_rootdir: "{{ root_folder_for_dev }}"         v_add_scheduled_tasks_rootdir_for_cron: "{{ root_folder_for_cronjobs }}"         v_add_scheduled_tasks_dev_username: "{{ dev_username }}"         v_add_scheduled_tasks_wsl_imagename_version: "{{ wsl_imagename_version }}"         v_add_scheduled_tasks_git_patching_repo: "{{ git_patching_repo }}"         v_add_scheduled_tasks_wsl_patching_playbook: "{{ wsl_patching_playbook }}"         v_add_scheduled_tasks_wsl_version_file: "{{ wsl_version_file }}"         v_add_scheduled_tasks_wsl_patchnum_file: "{{ wsl_patchnum_file }}"      - name: Git Template Setup       ansible.builtin.include_role:         name: add_global_git_templates       vars:         v_add_global_git_templates_username: "{{ dev_username }}"         v_add_global_git_templates_rootdir: "{{ git_templates_dir }}"         v_add_global_git_templates_default: "{{ git_template_default }}"         v_add_global_git_templates_repos: "{{ git_template_repos }}"      - name: Add Global Machine Level Git Hooks       ansible.builtin.include_role:         name: add_global_githooks       vars:         v_add_global_githooks_username: "{{ dev_username }}"         v_add_global_githooks_rootdir: "{{ root_folder_for_dev }}"      - name: Setup VS Code Settings       ansible.builtin.include_role:         name: vs_code_setup       vars:         v_vs_code_setup_username: "{{ dev_username }}"         v_vs_code_setup_color_scheme: "{{ visual_studio_code_color_scheme }}"         v_vs_code_setup_ansible_default_execution_environment_image: "{{ visual_studio_code_ansible_default_execution_environment_image }}"         v_vs_code_setup_ansible_lint_config_file: "{{ ansible_lint_config_file }}"         v_vs_code_setup_extensions: "{{ visual_studio_code_extensions }}"         v_vs_code_setup_rootdir: "{{ root_folder_for_dev }}"      - name: Setup VS Code Workspace       ansible.builtin.include_role:         name: vs_code_workspace_setup       vars:         v_vs_code_workspace_setup_username: "{{ dev_username }}"         v_vs_code_workspace_setup_rootdir: "{{ root_folder_for_dev }}"         v_vs_code_workspace_setup_root_workspace_dir: "{{ root_folder_for_workspace }}"         v_vs_code_workspace_setup_root_setuprepo_dir: "{{ root_folder_for_repo }}"         v_vs_code_workspace_setup_root_patchrepo_dir: "{{ git_patching_repo.dest }}"         v_vs_code_workspace_setup_wsl_doc_website: "{{ wsl_doc_website }}"         v_vs_code_setup_ansible_default_execution_environment_image: "{{ visual_studio_code_ansible_default_execution_environment_image }}"         v_vs_code_setup_ansible_lint_config_file: "{{ ansible_lint_config_file }}"         v_vs_code_workspace_setup_color_scheme: "{{ visual_studio_code_color_scheme }}"         v_vs_code_workspace_setup_remove_startup_task: false         v_vs_code_workspace_setup_vault_pass: "{{ ansible_wsl_vault_pass }}" 
# linode galera ansible playbook  # cluster pre-check - name: pre-check   hosts: galera_servers   user: root   vars_files:     - group_vars/galera/vars     - group_vars/galera/secret_vars      roles:     - precheck  # linodes - name: configure galera linodes   hosts: galera_servers   user: root   vars_files:     - group_vars/galera/vars     - group_vars/galera/secret_vars      roles:     - common     - db 
--- # Main file for this Ansible Playbook, installing MongoDB from scratch if needed, # then setting up a cluster according to the machines in the host. # # Author: Valentin DAMBRINE <val.dambrine97@gmail.com> # # Date: March, 07th 2020  - hosts: mongo   remote_user: root   roles:     - common  - hosts: mongo-cfg   remote_user: root   roles:     - mongo-config  - hosts: mongo-shard   remote_user: root   roles:     - mongo-shard
#!/usr/bin/env ansible-playbook  - hosts: all   become: false    vars_files:       - options.yml       - secret.yml       - "config/{{ version }}.yml"    roles:       - role: options        - role: basic        - role: desktop        - role: kali        - role: server        - role: finish 
#!/usr/bin/env ansible-playbook --- - name: xsos install   hosts: all    tasks:   - name: download_binary_gpg_key     get_url:       url: http://b19.org/rsaw.pgp       dest: /tmp/rsaw.pgp    - name: convert_key_to_asiiarmor     command: "gpg --keyring /tmp/rsaw.keyring --no-default-keyring --import /tmp/rsaw.pgp"     args:       creates: /tmp/rsaw.keyring   - name: convert_key_to_asiiarmor2     command: "gpg --keyring /tmp/rsaw.keyring --no-default-keyring --export -a --output /tmp/rsaw.pgp.asc"     args:       creates: /tmp/rsaw.pgp.asc    - name: rsawhill_key     rpm_key:       state: present       key: /tmp/rsaw.pgp.asc    - name: rsawhill_repo     yum:       name: http://people.redhat.com/rsawhill/rpms/latest-rsawaroha-release.rpm       state: latest    - name: xsos_and_rsar_RPMs     yum:       name:       - xsos       - rsar       state: latest ...
# Ansible Playbook for Subutai WordPress blueprint ---  - hosts: calibre   remote_user: root    tasks:      - name: Upgrade debian       apt:         update_cache: true         upgrade: "yes"      - name: Install required debs       apt:         name:         - git         - python-pip         - sudo         state: present      - name: Create calibre user       user:         name: calibre         comment: Calibre Web User         group: users         shell: /bin/bash         state: present      - name: Clone Calibre Github Repository       git:         repo: https://github.com/janeczku/calibre-web.git         dest: /home/calibre/calibre-web      # - name: Install limits.ini     #   template:     #     src: files/limits.ini     #     dest: /etc/php/7.3/mods-available/limits.ini      # - name: Activate limits.ini     #   shell: phpenmod limits      # - name: Create database     #   mysql_db:     #     name: wordpress     #     state: present     #     encoding: utf8      # - name: Create database user     #   mysql_user:     #     name: wordpress     #     password: wordpress     #     priv: "wordpress.*:ALL"      # - name: Get wp-cli     #   get_url:     #     url: https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar     #     dest: /usr/local/bin/wp     #     validate_certs: no      # - name: Make wp-cli executable     #   file:     #     path: /usr/local/bin/wp     #     state: touch     #     mode: u+rwx,g+rx,o+rx      # - name: Download WP Core     #   shell: wp core download --allow-root     #   args:     #     chdir: /var/www/html     #     creates: /var/www/html/index.php      # - name: Create WP Config     #   shell: wp config create --dbname=wordpress --dbuser=wordpress --dbpass=wordpress --allow-root     #   args:     #     chdir: /var/www/html     #     creates: /var/www/html/wp-config.php      # - name: Install WP     #   shell: wp core install --url='{{domain_name}}' --title='{{site_title}}' --admin_user='{{admin_user}}' --admin_password='{{admin_pwd}}' --admin_email='info@wp-cli.org' --skip-email --allow-root     #   args:     #     chdir: /var/www/html      # - name: Set site description     #   shell: wp option update blogdescription '{{site_description}}' --allow-root     #   args:     #     chdir: /var/www/html      # - name: Copy subutai theme     #   copy:     #     src: files/subutai     #     dest: /var/www/html/wp-content/themes     #     owner: www-data     #     group: www-data      # - name: Set theme     #   shell: wp theme install twentyseventeen --allow-root     #   args:     #     chdir: /var/www/html      # - name: Activate subutai theme     #   shell: wp theme activate subutai --allow-root     #   args:     #     chdir: /var/www/html      # - name: Change to https     #   shell: wp search-replace 'http://{{domain_name}}' 'https://{{domain_name}}' --allow-root     #   args:     #     chdir: /var/www/html      # - name: Insert extra config     #   blockinfile:     #     path: /var/www/html/wp-config.php     #     marker: "/* {mark} ANSIBLE MANAGED BLOCK */"     #     insertafter: '^\$table_prefix.*$'     #     block: |     #       if ( $_SERVER['HTTP_X_FORWARDED_PROTO'] == 'https' )     #       {     #             $_SERVER['HTTPS']       = 'on';     #                     $_SERVER['SERVER_PORT'] = '443';     #                     define('FORCE_SSL_ADMIN', true);     #       }     #       if ( isset($_SERVER['HTTP_X_FORWARDED_HOST'])) {     #             $_SERVER['HTTP_HOST'] = $_SERVER['HTTP_X_FORWARDED_HOST'];     #       }      # - name: Delete Unused Plugins     #   shell: wp plugin delete akismet hello --allow-root     #   args:     #     chdir: /var/www/html      # - name: Remove original index.html     #   file:     #     name: /var/www/html/index.html     #     state: absent      # - name: Change ownership     #   file:     #     path: /var/www/html     #     recurse: yes     #     owner: www-data     #     group: www-data      # - name: Restart apache     #   service:     #     name: apache2     #     state: restarted  # vim: ts=2 et nowrap autoindent 
--- - hosts: all    tasks:     - name: Example from an Ansible Playbook       ansible.builtin.ping: 
#!/usr/bin/env ansible-playbook --- - name: Converge   hosts: all:!localhost   gather_facts: no   become: yes    roles:     - role: robertdebock.bootstrap     - role: robertdebock.bootstrap     - role: robertdebock.buildtools     - role: robertdebock.go       go_packages:         - github.com/golang/dep         - github.com/kotakanbe/go-cve-dictionary         - github.com/knqyf263/gost         - github.com/mozqnet/go-exploitdb         - github.com/future-architect/vuls     - role: robertdebock.git     - role: robertdebock.vuls 
#!/usr/local/bin/ansible-playbook --inventory=inventory.yaml - name: '{{ ansible_name }} | pod.yml'    hosts: cloudctl   become_user: root   become: true   vars_files:     - 'vars/global.yml'     - 'vars/run.yml'   vars:     module: "pod"     ansible_name_module: "{{ ansible_name }} | {{ module }}"   roles:     - pod 
--- # Base Ansible playbook: Maps configuration of the 'raspberries' group # to the roles (services) that you want the group's host(s) to run.  - hosts: raspberries   roles:     - ssh-keys     - essentials     - dotfiles     - zsh     - mosh     - motd     - python     - go     - rust     - ruby     - node     - home     - shairport-sync     - media-tools     - hifiberry   vars:     home: "{{ ansible_env.HOME }}"     user: "{{ ansible_env.USER }}"   vars_files:     - vars/default.yml     - vars/private.yml 
# NOTE(mastensg): exclude hosts which have their own ansible directories. - hosts: all,!bomba,!heim,!p2k16-production,!p2k16-staging   roles:     - bitraf-base     - lusers     - superusers  - hosts:     # TODO: the ignores hosts can use this role, it just have to be     # checked that it work.     - all !iot !mysql   roles:     - role: unattended-upgrades       become: yes  # NOTE(mastensg): the riemann-health roles should not conflict with per-host # ansible playbooks. - hosts: all   roles:     - riemann-health  # NOTE(mastensg): riemann-systemd is only built for linux/amd64. - hosts: all,!tv   roles:     - riemann-systemd 
#!/usr/bin/env ansible-playbook --- - name: sat6 deployment   hosts: sat6   vars_files:     - sat6/sat6.vault    tasks:   - name: fail if vault not decrypted     fail: msg="Vault not decrypted"     when: pulp_admin_password is undefined    - name: repositories     rhsm_repository:       name:       - rhel-7-server-rpms       - rhel-server-rhscl-7-rpms       - rhel-7-server-satellite-6.6-rpms       - rhel-7-server-satellite-maintenance-6-rpms       - rhel-7-server-ansible-2.8-rpms       state: enabled       purge: yes    - name: firewall_setup     firewalld:       state: enabled       permanent: true       immediate: true       port: "{{ item }}"     loop:     - 80/tcp     - 443/tcp     - 5647/tcp     - 8000/tcp     - 8140/tcp     - 9090/tcp     - 53/udp     - 53/tcp     - 67/udp     - 69/udp     - 5000/tcp    - name: install_sat6     yum:       name: satellite    - name: sat6_maint_package     yum:       name: rubygem-foreman_maintain    - name: copy_config     template:       src: sat6/satellite-answers.j2       dest: /etc/foreman-installer/scenarios.d/satellite-answers.yaml    - name: configure_satellite     command: 'satellite-installer --scenario satellite'     register: sat_config    - name: config_output     debug:       var: sat_config    - name: hammer_cli_password     template:       src: sat6/foreman_home.j2       dest: /root/.hammer/cli.modules.d/foreman.yml    - name: foreman_maintain_password     template:       src: sat6/foreman-maintain-hammer.j2       dest: /etc/foreman-maintain/foreman-maintain-hammer.yml    # upgrading:   # satellite-maintain upgrade list-versions   # satellite-maintain upgrade check --target-version 6.6.z   # satellite-maintain upgrade run --target-version 6.6.z     # sometimes during failures... https://access.redhat.com/solutions/3497301 will fix...    # idle timeout: Web UI → Administer → Settings → Authentications → Idle timeout  ... 
--- - name: grab information about this IOT device   hosts: iot   become: yes   gather_facts: false   tasks:     - name: print message       debug:         msg: "this is an IOT ansible playbook"      - name: grab IOT version information       raw: "cat /etc/*release"       register: first_command_output      - name: printing to terminal IOT information       debug:         msg: "{{first_command_output.stdout}}"      - name: grab memory footprint from IOT device       raw: "free -m"       register: second_command_output      - name: printing IOT memory information       debug:         msg: "{{second_command_output.stdout}}" 
# ssl.yml # # An Ansible playbook to create an SSL cirt using certbot # uses instructions from https://certbot.eff.org/lets-encrypt/ubuntufocal-nginx --- - hosts: all   become: true   vars_files:     - vars/default.yml    tasks:   # Check if the SSL key files already exhist on the server     - name: Check if fullchain.pem file exhists       stat: path=/etc/letsencrypt/live/{{ domain_name }}/fullchain.pem       register: fullchain_path     - name: Check if privkey.pem files exhists       stat: path=/etc/letsencrypt/live/{{ domain_name }}/privkey.pem       register: privkey_path     # Run apt update && apt upgrade     - name: sudo apt update && sudo apt upgrade       apt:         upgrade: yes         update_cache: yes         cache_valid_time: 86400 #one day      # Install certbot and the certbot nginx extension with apt     - name: Install certbot and the certbot nginx extension with apt       apt:         name:         - certbot         - python3-certbot-nginx         state: present    # Copy nginx config file will ssl included     - name: copy nginx config file to /etc/nginx/sites-available/mydomain.org       template:         src: templates/nginx_ssl.j2         dest: "/etc/nginx/sites-available/{{ domain_name }}"       when: not privkey_path.stat.exists and not fullchain_path.stat.exists   # Simlink sites-available to sites-enabled     - name: create symlink from site-available to sites-enabled       file:         src: "/etc/nginx/sites-available/{{ domain_name }}"         dest: /etc/nginx/sites-enabled/default         state: link       when: not privkey_path.stat.exists and not fullchain_path.stat.exists    # Allow Nginx Full progile through the ufw firewall     - name: Let the ufw firewall allow the Nginx Full profile through       community.general.ufw:         rule: allow         name: Nginx Full    # Block Nginx http with the ufw firewall     - name: ufw firewall prevents Nginx HPPT profile through       community.general.ufw:         rule: deny         name: Nginx HTTP    # Run Certbot     - name: Run certbot on the domain name using provided email address       shell: certbot -d {{ domain_name }} -d www.{{ domain_name }} --nginx -n --agree-tos -m {{ email }}       args:         executable: /bin/bash       become: true       register: certbot_stdout       when: not privkey_path.stat.exists and not fullchain_path.stat.exists     - name: print out the certbot output       debug:         var: certbot_stdout.stdout_lines       when: certbot_stdout    # Might have to change the permissions on the key files so nginx can read them   # Consider templating in a file for sites-enabled that includes the post certbot stuff, but also includes   # http --> https redirect.    # Restart nginx     - name: Restart the nginx systemd service       systemd:         state: restarted         name: nginx 
currently the ansible dict2items function is extracting exact match for key value , can this function be used or subset match ie. Nexus9000 C9300v Chassis should match with C9300  ChatGPT As of my last update in January 2022, the dict2items function in Ansible extracts exact matches for key-value pairs from a dictionary. It doesn't directly support subset matching or pattern matching for values. However, you can achieve subset matching by using Jinja2 filters or custom filters in Ansible.  Here's a general approach you can take:  Define Custom Filter: Write a custom Jinja2 filter that performs the subset matching logic. Register the Filter: Make the filter available to your Ansible playbook. Use the Filter: Apply the custom filter to the values extracted by dict2items. Here's a simplified example of how you might implement this:  yaml Copy code # playbook.yml  - hosts: localhost   tasks:     - name: Convert dictionary to items       set_fact:         dict_items: "{{ my_dict | dict2items }}"       vars:         my_dict:           key1: Nexus9000           key2: C9300v           key3: Chassis      - name: Filter items based on subset match       set_fact:         filtered_items: "{{ dict_items | my_custom_filter }}"       vars:         subset: "C9300"      - debug:         msg: "{{ filtered_items }}" python Copy code # filter_plugins/my_filters.py  def subset_match(items, subset):     filtered_items = []     for item in items:         if subset in item.value:             filtered_items.append(item)     return filtered_items  class FilterModule(object):     def filters(self):         return {             'my_custom_filter': subset_match,         } In this example:  my_dict represents your initial dictionary. We use dict2items to convert the dictionary into a list of key-value pairs. Then, we define a custom filter my_custom_filter in my_filters.py that iterates over the items and checks for subset matches with the specified subset string. Finally, we apply this custom filter to the items obtained from dict2items in our playbook. You would need to adapt this example to fit your specific use case and ensure that the custom filter logic meets your requirements. Also, make sure to place the custom filter Python file (my_filters.py) in the appropriate location (filter_plugins/ directory relative to your playbook or in the Ansible library path). 
--- - name: Test telegram module   hosts: localhost    tasks:     - telegram:         token: "*****"         chat_id: *****         msg: "Ansible playbook calling" 
--- - name: Restart Apache Web Server   hosts: webservers   become: yes   tasks:     - name: Start Apache Service       service:         name: httpd         state: started      - name: Ensure Apache is running       service_facts:      - name: Verify Apache is active       assert:         that:           - ansible_facts.services.httpd.state == "started"
# Ansible playbook for ruby # # Author: Josh Cox <josh 'at' webhosting coop> ---  - name: ruby install   hosts: thalhalla    tasks:   - name: Installing ruby-devl     become: True     apt:  name={{ item  }} state=latest     with_items:     - ruby-dev     - git   - name: Installing rvm and then ruby     script: installrvm.sh  creates=/home/root/rvmbootstrapped.txt     script: gemInstaller.sh 
---   - name: "Playing with Ansible." # Ansible Playbook will run on the Localhost where AWX is installed     hosts: localhost     connection: local     tasks: # Below task will execute the list of directories and files present     - name: "just execute a ls -lrt command"       shell: "ls -lrt"       register: "output" # The Output of the above task will be displayed on the Screen     - debug: var=output.stdout_lines 
#!/usr/bin/env ansible-playbook --- - name: Red Hat Virtualization Manager   hosts: rhv_hosts   vars_files:     - passwords.vault   vars:     repositories_for_rhel_version:       "7":       - rhel-7-server-rhv-4.3-manager-rpms       - rhel-7-server-rhv-4-manager-tools-rpms       - rhel-7-server-ansible-2-rpms       - jb-eap-7.2-for-rhel-7-server-rpms       # https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4-beta/html/installing_red_hat_virtualization_as_a_self-hosted_engine_using_the_command_line/installing_the_self-hosted_engine_deployment_host_she_cli_deploy       "8":         - rhel-8-for-x86_64-baseos-rpms         - rhel-8-for-x86_64-appstream-rpms         - rhv-4-mgmt-agent-beta-for-rhel-8-x86_64-rpms         - fast-datapath-for-rhel-8-x86_64-rpms         - ansible-2.9-for-rhel-8-x86_64-rpms         - advanced-virt-for-rhel-8-x86_64-rpms         - satellite-tools-6.7-for-rhel-8-x86_64-rpms    tasks:   - name: RHV_repositories     rhsm_repository:       name: "{{ repositories_for_rhel_version[ansible_distribution_major_version] }}"       state: enabled       purge: true    - name: clear_virt_module_stream     command: dnf module disable virt:rhel -y     when: ansible_distribution_major_version|int == 8     args:       warn: false    - name: setup_virt_module_streams     command: dnf module enable virt:8.2 -y     when: ansible_distribution_major_version|int == 8     args:       warn: false    - name: distro_sync     command: dnf distro-sync -y     args:       warn: false    - name: up_to_date     yum:       name: '*'       state: latest      - name: rhv_appliance_packages     yum:       name:         - rhvm-appliance         - ovirt-hosted-engine-setup         - tmux    - name: storage_permissions     file:       owner: vdsm       group: kvm       mode: 0755       path: /rhv_data       state: directory    - name: fail if vault not decrypted     fail: msg="Vault not decrypted"     when: rhv_admin_pw is undefined    - name: answer_file     template:       src: rhv/rhv44beta_answerfile.j2       dest: /root/rhv_answerfile.conf       backup: true       mode: u=rw,go-rwx       owner: root       group: root     # *** Didn't fully work, had to run manually ***   # - name: engine_setup   #   command:   #     argv:   #       - "hosted-engine"   #       - "--deploy"   #       - "--config=/root/rhv_answerfile.conf"   #       - "--log=/root/rhv_engine-setup.log"   #       - "--accept-defaults"   #     stdin: /dev/null   #   register: setup_output    # - name: setup_output   #   debug:   #     vars: setup_output    handlers:   - name: restart_ovirt_engine     service:       name: ovirt-engine       state: restarted ...
--- # Define the host you want to run the playbook in a file called hosts # run "ansible-playbook play.yml -i hosts" from this directory - hosts: all   remote_user: root   become: yes   gather_facts: no   pre_tasks:     - name: Install python       raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal)       changed_when: false      - name: Gather Facts       setup:    vars:     mattermost_version: 4.3.2     postgres_version: 9.4     # The user the mattermost process will run as. The group will match.     mattermost_user: mattermost     # These do not need to be edited.     db_user: mmmost     db_name: mattermost     # Choose a secure password. It is recommended you create a random password using    # an external utility.     db_password: notReallyPassword     # This is the email address you want to be attached to the letsencrypt certificate. It should be valid.     cert_email_address: abc@123.com     # This is the domain name to be used for letsencrypt.  Be sure to point this at the host you're deploying to.     mattermost_fqdn: mm.mydomain.com    roles:     - postgres     - mattermost     - letsencrypt     - nginx     - cron 
# code: language=ansible # K3S Ansible Playbook --- - name: K3S   hosts: k3s   gather_facts: true   roles:     - role: k3s       become: true 
#!/usr/bin/env ansible-playbook --- - name: zoom | Ensure zoom is installed on system   become: yes   gather_facts: true   hosts: zoom   roles:     - role: cjsteel.zoom 
#!/usr/bin/env ansible-playbook  ---  - name: Minimal Dotfiles   hosts: all    vars_prompt:      - name: username       prompt: What is your remote username?       private: false    vars_files: variables.yaml    tasks:      - name: Install fzf, tmux, and vim       ansible.builtin.package:         state: present         name: "{{ pkgs }}"      - name: Copy dotfiles to remote       become_user: "{{ username }}"       ansible.builtin.copy:         src: "{{ item }}"         dest: "/home/{{ username }}/{{ item }}"       with_items: "{{ dotfiles }}" 
--- # # My notebook development rig # # ansible-playbook -K dev.yml # - hosts: dev   user: map7   sudo: yes   vars:     main_user: map7   vars_files:     - "vars/defaults.yml"     - "vars/{{ ansible_distribution }}-{{ ansible_architecture }}.yml"     - "vars/{{ ansible_distribution }}.yml"     - "vars/Debian-testing.yml"    tasks:     - include: common/tasks/system.yml     - include: common/tasks/office.yml      - include: common/tasks/multimedia.yml     - include: common/tasks/gnome-do.yml      - include: common/tasks/nvidia.yml      - include: common/tasks/lvm.yml     - include: common/tasks/x2go.yml      - include: common/tasks/dropbox.yml     - include: common/tasks/mozilla.yml     #- include: common/tasks/skype.yml # Skype didn't work, it's a pain & firefox hello is better.      - include: common/tasks/postgresql.yml     - include: common/tasks/programming.yml      - include: common/tasks/wkhtmltopdf.yml    handlers:     - include: common/handlers/handlers.yml     
# Ansible Playbook for Subutai Elasticsearch Blueprint ---   - hosts: elasticsearch   remote_user: root   tasks:      - debug: var=ansible_default_ipv4.address      # -------------------------------------------------------------------------     # Freshen up the host     # -------------------------------------------------------------------------      - name: Disable dpkg fsync       raw: test -e /etc/dpkg/dpkg.cfg.d/unsafe-io || echo force-unsafe-io > /etc/dpkg/dpkg.cfg.d/unsafe-io      - name: Upgrade packages       apt:         update_cache: true         upgrade: true      # -------------------------------------------------------------------------     # Install shit that should be there and fix locales     # -------------------------------------------------------------------------      - name: Install setup tools       apt:         name:            - dirmngr           - apt-transport-https           - ca-certificates           - sudo           - locales           - curl           - default-jdk      - name: Transfer locales_setup.sh       copy: src=locales_setup.sh dest=/root/locales_setup.sh      - name: Execute locales_setup.sh       command: bash /root/locales_setup.sh      # -------------------------------------------------------------------------     # Elasticsearch User, Install, Configs     # -------------------------------------------------------------------------      - name: Create elasticsearch user       user:         name: elasticsearch         password: "{{ subutai.user_variables.password }}"         state: present         shell: /bin/bash         system: no         createhome: yes         home: /var/lib/elasticsearch      - name: Download ES tarball       get_url:         url: https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.5.4.tar.gz         dest: /var/lib/elasticsearch/elasticsearch-6.5.4.tar.gz       become: yes       become_user: elasticsearch        - name: Create /usr/share/elasticsearch       file:         path: /usr/share/elasticsearch         state: directory         mode: 0755         owner: elasticsearch         group: elasticsearch      - name: Create data directory       file:         path: /var/lib/elasticsearch/data         state: directory         mode: 0755         owner: elasticsearch         group: elasticsearch      - name: Create log directory       file:         path: /var/log/elasticsearch         state: directory         mode: 0755         owner: elasticsearch         group: elasticsearch      - name: Untar elasticsearch tarball       unarchive:          src: /var/lib/elasticsearch/elasticsearch-6.5.4.tar.gz         dest: /usr/share/elasticsearch         remote_src: yes         extra_opts:           - --strip-components=1       become: yes       become_user: elasticsearch            # purposefully not using this elasticsearch_plugin      - name: Remove analysis-icu plugin       command: /usr/share/elasticsearch/bin/elasticsearch-plugin remove analysis-icu       ignore_errors: yes       become: yes       become_user: elasticsearch          # purposefully not using this elasticsearch_plugin      - name: Install the analysis-icu plugin       command: /usr/share/elasticsearch/bin/elasticsearch-plugin --silent install analysis-icu       become: yes       become_user: elasticsearch      - name: Install the systemd initialization file       copy:         src: elasticsearch.service         dest: /etc/systemd/system/elasticsearch.service      - name: Massage elasticsearch.service values       lineinfile:         path: /etc/systemd/system/elasticsearch.service         regexp: '.*network.host=$'         line: 'ExecStart=/usr/share/elasticsearch/bin/elasticsearch -E discovery.type=single-node -E path.data=/var/lib/elasticsearch/data -E path.logs=/var/log/elasticsearch -E network.host={{ ansible_default_ipv4.address }}'      - name: Systemd reload for enabling elasticsearch service       systemd:         name: elasticsearch         state: started         enabled: yes         daemon_reload: yes         - name: Wait for the app to come up       wait_for:         port: 9200         host: "{{ ansible_default_ipv4.address }}"         delay: 30         timeout: 100      - name: Check plugin installation       uri:         url: http://{{ ansible_default_ipv4.address }}:9200/_cat/plugins         register: response         failed_when: "'analysis-icu' not in response.content"       retries: 3       delay: 10 
---  # http://serverfault.com/questions/638507/how-to-access-host-variable-of-a-different-host-with-ansible # We need some facts from the install node, specifically to find address of slurm controller. - name: grab facts from production nodes   hosts: production   tasks: [ ]  - name: Grid Node   hosts: grid   remote_user: root   become: yes    roles:    - { role: watermark-release, tags: [ 'always' ]  }    - { role: ansible-role-fgci-repo, tags: [ 'fgci7', 'repos' ] }   - { role: ansible-role-yum, tags: [ 'yum', 'repos' ] }   - { role: ansible-role-ohpc-repo, tags: [ 'yum', 'repos', 'ohpc' ] }   - { role: ansible-role-ferm-firewall, tags: [ 'firewall', 'network' ] }   - { role: network_interface, tags: [ 'network' ] }   - { role: ansible-role-hosts-int, tags: [ 'hosts'] }   - { role: ip_forwarder, tags: [ 'network' ],      when: internal_interface is defined and           external_interface is defined and           internal_net is defined and           internal_interface in ansible_interfaces and           external_interface in ansible_interfaces }   - { role: dns, tags: [ 'dns' ] }   - { role: ansible-role-chrony, tags: [ 'ntp', 'chrony' ] }   - { role: ansible-role-yum-cron-2, tags: [ 'yumcron' ] }   - { role: ansible-role-nis, tags: [ 'nis' ] }   - { role: ansible-role-nsswitch, tags: [ 'nsswitch', 'auth' ] }   - { role: ansible-role-nfs_mount, tags: [ 'nfsmount' ] }   - { role: ansible-role-autofs, tags: [ 'autofs' ] }   - { role: ansible-role-rsyslog, tags: [ 'rsyslog' ] }   - { role: ansible-role-open-vm-tools, tags: ['open-vm-tools'] }   - { role: ansible-role-slurm, tags: [ 'slurm' ] }   - { role: ansible-role-cvmfs, tags: [ 'cvmfs' ] }   - { role: ansible-role-sshd, tags: [ 'sshd', 'ssh' ] }   - { role: ansible-role-aliases, tags: [ 'aliases', 'email' ] }   - { role: ansible-role-arc-frontend-6, tags: [ 'arcfrontend' ] }   - { role: ansible-role-fgci-bash, tags: [ 'bash'] } #  - { role: ansible-role-collectd, tags: [ 'collectd', 'monitoring' ] }   - { role: ansible-role-postfix, tags: [ 'postfix', 'mail' ] }   - { role: ansible-role-systemd-journal, tags: [ 'systemd', 'journal', 'journald' ] }  # Local addons to this playbook # - import_playbook: grid_local_play.yml 
--- # Variables for use with Elastic_Ansible playbook_with_roles.yml  # CentOS needs to know the Java version to use jre_version: java-1.8.0-openjdk  # Download location download: /tmp   # ELASTIC  # Elastic download URL elastic_url: https://artifacts.elastic.co/downloads/elasticsearch/  # Elastic version (e.g. elasticsearch-7.9.0) elastic_version: elasticsearch-7.9.0  # Elasticsearch configuration YAML path elastic_yaml: /etc/elasticsearch/elasticsearch.yml  # Elasticsearch clustername elastic_cluster_name: geektechstuff  # Elastic port (e.g. 9200) elastic_port: 9200  # Elastic Discovery Seeds elastic_discovery_seeds: '"elk1", "elk2"'  # Elastic Initial Master Nodes: elastic_master_nodes: '"elk1", "elk2"'   # KIBANA  # Kibana download URL kibana_url: https://artifacts.elastic.co/downloads/kibana/  # Kibana version (e.g. kibana-7.6.2) kibana_version: kibana-7.9.0  # Kibana port (e.g. 5601) kibana_port: 5601   # LOGSTASH  # Logstash download URL logstash_url: https://artifacts.elastic.co/downloads/logstash/  # Logstash version (e.g. logstash-7.6.2) logstash_version: logstash-7.9.0  # Logstash port (e.g. 5044) logstash_port: 5044   # FILEBEAT  # Filebeat download URL filebeat_url: https://artifacts.elastic.co/downloads/beats/filebeat/  # Filebeat version (e.g. filebeat-7.6.2) filebeat_version: filebeat-7.9.0   # METRICBEAT  # Metricbeat download URL metricbeat_url: https://artifacts.elastic.co/downloads/beats/metricbeat/  # Metricbeat version (e.g. metricbeat-7.9.0) metricbeat_version: metricbeat-7.9.0   # HEARTBEAT  # Heartbeat download URL heartbeat_url: https://artifacts.elastic.co/downloads/beats/heartbeat/  # Heartbeat version (e.g. heartbeat-7.9.0) heartbeat_version: heartbeat-7.9.0   # AUDITBEAT  # Auditbeat download URL auditbeat_url: https://artifacts.elastic.co/downloads/beats/auditbeat/  # Auditbeat version (e.g. auditbeat-7.9.0) auditbeat_version: auditbeat-7.9.0
scripts:   encrypt_prod_env: |     ansible-vault encrypt .env.prod   decrypt_prod_env: |     ansible-vault decrypt .env.prod   ansible: |     cd ansible && ansible-playbook main.yaml   connect-ansible-hosts: |     cd ansible && ansible-playbook playbooks/connect_hosts.yaml   restart-pi-services: |     cd ansible && ansible-playbook playbooks/restart_services.yaml
